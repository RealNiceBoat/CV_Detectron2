{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://detectron2.readthedocs.io/\n",
    "import detectron2\n",
    "import logging\n",
    "import torch\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "logger = logging.getLogger('detectron2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "from pathlib import Path\n",
    "base_folder = Path('.')\n",
    "data_folder = base_folder/'til2020'\n",
    "train_imgs_folder = data_folder/'train'\n",
    "train_annotations = data_folder/'train.json'\n",
    "val_imgs_folder = data_folder/'val'\n",
    "val_annotations = data_folder/'val.json'\n",
    "test_imgs_folder = data_folder/'CV_interim_images'\n",
    "\n",
    "#keep these the same\n",
    "save_model_folder = base_folder/'ckpts'\n",
    "load_model_folder = base_folder/'ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register datasets. now they can be used as if they were native. remember to run fix_annotations.py to convert TIL2020 to proper COCO format\n",
    "#to implement custom loaders, such as pickled, https://detectron2.readthedocs.io/modules/data.html?highlight=DatasetCatalog#detectron2.data.DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"til_train\", {}, train_annotations, train_imgs_folder)\n",
    "register_coco_instances(\"til_val\", {}, val_annotations, val_imgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/15 15:47:24 detectron2]: \u001b[0mLoaded Config:\nCUDNN_BENCHMARK: false\nDATALOADER:\n  ASPECT_RATIO_GROUPING: true\n  FILTER_EMPTY_ANNOTATIONS: true\n  NUM_WORKERS: 12\n  REPEAT_THRESHOLD: 0.6\n  SAMPLER_TRAIN: RepeatFactorTrainingSampler\nDATASETS:\n  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n  PROPOSAL_FILES_TEST: []\n  PROPOSAL_FILES_TRAIN: []\n  TEST:\n  - til_val\n  TRAIN:\n  - til_train\nEPOCH_SIZE: 2741\nGLOBAL:\n  HACK: 1.0\nINPUT:\n  CROP:\n    ENABLED: false\n    SIZE:\n    - 0.8\n    - 0.9\n    TYPE: relative_range\n  FORMAT: BGR\n  MASK_FORMAT: polygon\n  MAX_SIZE_TEST: 1333\n  MAX_SIZE_TRAIN: 1333\n  MIN_SIZE_TEST: 800\n  MIN_SIZE_TRAIN:\n  - 640\n  - 672\n  - 704\n  - 736\n  - 768\n  - 800\n  MIN_SIZE_TRAIN_SAMPLING: choice\n  RAND_BRIGHTNESS:\n  - 0.95\n  - 1.05\n  RAND_CONTRAST:\n  - 0.95\n  - 1.05\n  RAND_SATURATION:\n  - 0.95\n  - 1.05\nMODEL:\n  ANCHOR_GENERATOR:\n    ANGLES:\n    - - -90\n      - 0\n      - 90\n    ASPECT_RATIOS:\n    - - 0.5\n      - 1.0\n      - 2.0\n    NAME: DefaultAnchorGenerator\n    OFFSET: 0.0\n    SIZES:\n    - - 32\n    - - 64\n    - - 128\n    - - 256\n    - - 512\n  BACKBONE:\n    FREEZE_AT: 2\n    NAME: build_resnet_fpn_backbone\n  DEVICE: cuda\n  FPN:\n    FUSE_TYPE: sum\n    IN_FEATURES:\n    - res2\n    - res3\n    - res4\n    - res5\n    NORM: ''\n    OUT_CHANNELS: 256\n  KEYPOINT_ON: false\n  LOAD_PROPOSALS: false\n  MASK_ON: false\n  META_ARCHITECTURE: GeneralizedRCNN\n  PANOPTIC_FPN:\n    COMBINE:\n      ENABLED: true\n      INSTANCES_CONFIDENCE_THRESH: 0.5\n      OVERLAP_THRESH: 0.5\n      STUFF_AREA_LIMIT: 4096\n    INSTANCE_LOSS_WEIGHT: 1.0\n  PIXEL_MEAN:\n  - 103.53\n  - 116.28\n  - 123.675\n  PIXEL_STD:\n  - 1.0\n  - 1.0\n  - 1.0\n  PROPOSAL_GENERATOR:\n    MIN_SIZE: 0\n    NAME: RPN\n  RESNETS:\n    DEFORM_MODULATED: false\n    DEFORM_NUM_GROUPS: 1\n    DEFORM_ON_PER_STAGE:\n    - false\n    - false\n    - false\n    - false\n    DEPTH: 101\n    NORM: FrozenBN\n    NUM_GROUPS: 1\n    OUT_FEATURES:\n    - res2\n    - res3\n    - res4\n    - res5\n    RES2_OUT_CHANNELS: 256\n    RES5_DILATION: 1\n    STEM_OUT_CHANNELS: 64\n    STRIDE_IN_1X1: true\n    WIDTH_PER_GROUP: 64\n  RETINANET:\n    BBOX_REG_WEIGHTS: &id002\n    - 1.0\n    - 1.0\n    - 1.0\n    - 1.0\n    FOCAL_LOSS_ALPHA: 0.25\n    FOCAL_LOSS_GAMMA: 2.0\n    IN_FEATURES:\n    - p3\n    - p4\n    - p5\n    - p6\n    - p7\n    IOU_LABELS:\n    - 0\n    - -1\n    - 1\n    IOU_THRESHOLDS:\n    - 0.4\n    - 0.5\n    NMS_THRESH_TEST: 0.5\n    NUM_CLASSES: 80\n    NUM_CONVS: 4\n    PRIOR_PROB: 0.01\n    SCORE_THRESH_TEST: 0.05\n    SMOOTH_L1_LOSS_BETA: 0.1\n    TOPK_CANDIDATES_TEST: 1000\n  ROI_BOX_CASCADE_HEAD:\n    BBOX_REG_WEIGHTS:\n    - &id001\n      - 10.0\n      - 10.0\n      - 5.0\n      - 5.0\n    - - 20.0\n      - 20.0\n      - 10.0\n      - 10.0\n    - - 30.0\n      - 30.0\n      - 15.0\n      - 15.0\n    IOUS:\n    - 0.5\n    - 0.6\n    - 0.7\n  ROI_BOX_HEAD:\n    BBOX_REG_WEIGHTS: *id001\n    CLS_AGNOSTIC_BBOX_REG: false\n    CONV_DIM: 256\n    FC_DIM: 1024\n    NAME: FastRCNNConvFCHead\n    NORM: ''\n    NUM_CONV: 0\n    NUM_FC: 2\n    POOLER_RESOLUTION: 7\n    POOLER_SAMPLING_RATIO: 0\n    POOLER_TYPE: ROIAlignV2\n    SMOOTH_L1_BETA: 0.0\n    TRAIN_ON_PRED_BOXES: false\n  ROI_HEADS:\n    BATCH_SIZE_PER_IMAGE: 512\n    IN_FEATURES:\n    - p2\n    - p3\n    - p4\n    - p5\n    IOU_LABELS:\n    - 0\n    - 1\n    IOU_THRESHOLDS:\n    - 0.5\n    NAME: StandardROIHeads\n    NMS_THRESH_TEST: 0.5\n    NUM_CLASSES: 5\n    POSITIVE_FRACTION: 0.25\n    PROPOSAL_APPEND_GT: true\n    SCORE_THRESH_TEST: 0.05\n  ROI_KEYPOINT_HEAD:\n    CONV_DIMS:\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    LOSS_WEIGHT: 1.0\n    MIN_KEYPOINTS_PER_IMAGE: 1\n    NAME: KRCNNConvDeconvUpsampleHead\n    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n    NUM_KEYPOINTS: 17\n    POOLER_RESOLUTION: 14\n    POOLER_SAMPLING_RATIO: 0\n    POOLER_TYPE: ROIAlignV2\n  ROI_MASK_HEAD:\n    CLS_AGNOSTIC_MASK: false\n    CONV_DIM: 256\n    NAME: MaskRCNNConvUpsampleHead\n    NORM: ''\n    NUM_CONV: 4\n    POOLER_RESOLUTION: 14\n    POOLER_SAMPLING_RATIO: 0\n    POOLER_TYPE: ROIAlignV2\n  RPN:\n    BATCH_SIZE_PER_IMAGE: 256\n    BBOX_REG_WEIGHTS: *id002\n    BOUNDARY_THRESH: -1\n    HEAD_NAME: StandardRPNHead\n    IN_FEATURES:\n    - p2\n    - p3\n    - p4\n    - p5\n    - p6\n    IOU_LABELS:\n    - 0\n    - -1\n    - 1\n    IOU_THRESHOLDS:\n    - 0.3\n    - 0.7\n    LOSS_WEIGHT: 1.0\n    NMS_THRESH: 0.7\n    POSITIVE_FRACTION: 0.5\n    POST_NMS_TOPK_TEST: 1000\n    POST_NMS_TOPK_TRAIN: 1000\n    PRE_NMS_TOPK_TEST: 1000\n    PRE_NMS_TOPK_TRAIN: 2000\n    SMOOTH_L1_BETA: 0.0\n  SEM_SEG_HEAD:\n    COMMON_STRIDE: 4\n    CONVS_DIM: 128\n    IGNORE_VALUE: 255\n    IN_FEATURES:\n    - p2\n    - p3\n    - p4\n    - p5\n    LOSS_WEIGHT: 1.0\n    NAME: SemSegFPNHead\n    NORM: GN\n    NUM_CLASSES: 54\n  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-101.pkl\nOUTPUT_DIR: ckpts\nPRINT_EVERY: 200\nSAVE_EVERY: 1000\nSEED: -1\nSOLVER:\n  BASE_LR: 0.00025\n  BIAS_LR_FACTOR: 1.0\n  CHECKPOINT_PERIOD: 5000\n  CLIP_GRADIENTS:\n    CLIP_TYPE: value\n    CLIP_VALUE: 1.0\n    ENABLED: false\n    NORM_TYPE: 2.0\n  EPOCHS: 50\n  GAMMA: 0.5\n  IMS_PER_BATCH: 3\n  LR_SCHEDULER_NAME: WarmupMultiStepLR\n  MAX_ITER: -1\n  MOMENTUM: 0.9\n  NESTEROV: false\n  STEPS:\n  - 10000000\n  WARMUP_FACTOR: 0.001\n  WARMUP_ITERS: 1000\n  WARMUP_METHOD: linear\n  WEIGHT_DECAY: 0.0001\n  WEIGHT_DECAY_BIAS: 0.0001\n  WEIGHT_DECAY_NORM: 0.0\nTEST:\n  AUG:\n    ENABLED: false\n    FLIP: true\n    MAX_SIZE: 4000\n    MIN_SIZES:\n    - 400\n    - 500\n    - 600\n    - 700\n    - 800\n    - 900\n    - 1000\n    - 1100\n    - 1200\n  DETECTIONS_PER_IMAGE: 100\n  EVAL_PERIOD: 0\n  EXPECTED_RESULTS: []\n  KEYPOINT_OKS_SIGMAS: []\n  PRECISE_BN:\n    ENABLED: false\n    NUM_ITER: 200\nVERSION: 2\nVIS_PERIOD: 0\n\n"
    }
   ],
   "source": [
    "#https://detectron2.readthedocs.io/modules/config.html\n",
    "#btw, i added some custom config options for my checkpointer and pipeline\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "#cfg.SEED = 42\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"til_train\",)\n",
    "cfg.DATASETS.TEST = (\"til_val\",)\n",
    "cfg.OUTPUT_DIR = str(save_model_folder)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 12\n",
    "cfg.DATALOADER.SAMPLER_TRAIN = \"RepeatFactorTrainingSampler\" #deals with class imbalance\n",
    "cfg.DATALOADER.REPEAT_THRESHOLD = 0.5\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2 #batch_size\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.GAMMA = 0.5 #halves the learning rate at each milestone\n",
    "cfg.SOLVER.STEPS = (10000000,)#(20000,60000,100000,140000) # milestones in iterations\n",
    "#^I played with these but realised it had little effect: so keep BASE_LR at 0.000125*IMS_PER_BATCH\n",
    "\n",
    "#Pipeline augmentation settings (i implemented these)\n",
    "cfg.INPUT.CROP.ENABLED = False\n",
    "cfg.INPUT.CROP.SIZE = [0.7, 0.9]\n",
    "#cfg.INPUT.RAND_ROTATION = [-2,2]\n",
    "cfg.INPUT.RAND_CONTRAST = [0.8,1.2]\n",
    "cfg.INPUT.RAND_BRIGHTNESS = [0.8,1.2]\n",
    "cfg.INPUT.RAND_SATURATION = [0.8,1.2]\n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5 #number of categories\n",
    "cfg.PRINT_EVERY = 200\n",
    "cfg.SAVE_EVERY = 1000 #when using ValCheckpointer, it saves only if val loss is the minimum so far\n",
    "\n",
    "cfg.SOLVER.MAX_ITER = -1\n",
    "cfg.SOLVER.EPOCHS = 50 #facebook uses the paradigm of endless datastreams, as such epochs dont really exist\n",
    "cfg.EPOCH_SIZE = int(8225/cfg.SOLVER.IMS_PER_BATCH)\n",
    "\n",
    "logger.info(f\"Loaded Config:\\n{cfg.dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (6): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (7): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (8): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (9): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (10): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (11): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (12): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (13): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (14): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (15): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (16): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (17): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (18): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (19): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (20): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (21): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (22): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n    )\n  )\n)\n"
    }
   ],
   "source": [
    "from detectron2.modeling import build_model\n",
    "model = build_model(cfg)\n",
    "logger.info(f\"Created Model:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import DatasetPipeline\n",
    "\n",
    "#uses whatever pycocotools is installed, make sure it is the TIL one\n",
    "def do_test(cfg,model):\n",
    "    from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "    from detectron2.data import build_detection_test_loader\n",
    "    evaluator = COCOEvaluator(cfg.DATASETS.TEST[0],cfg,False)\n",
    "    val_loader = build_detection_test_loader(cfg,cfg.DATASETS.TEST[0])\n",
    "    inference_on_dataset(model,val_loader,evaluator)\n",
    "\n",
    "def do_eval(cfg,model):\n",
    "    from detectron2.data import build_detection_test_loader,DatasetMapper\n",
    "    from tqdm import tqdm\n",
    "    dataloader = build_detection_test_loader(cfg,cfg.DATASETS.TEST[0],mapper=DatasetPipeline(cfg,False))\n",
    "    total_loss = 0\n",
    "    logger.info(\"Calculating Validation Loss...\")\n",
    "    with torch.no_grad():\n",
    "        for iteration,data in enumerate(tqdm(dataloader)):\n",
    "            loss_dict = model(data)\n",
    "            total_loss += sum(loss_dict.values())\n",
    "    tqdm.write(\"\\n\")\n",
    "    return total_loss/len(dataloader)\n",
    "\n",
    "def do_train(cfg,model,model_context,resume=False):\n",
    "    from detectron2.utils.events import (\n",
    "        CommonMetricPrinter,\n",
    "        EventStorage,\n",
    "        JSONWriter,\n",
    "        TensorboardXWriter,\n",
    "    )\n",
    "    import detectron2.utils.comm as comm\n",
    "    from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "    from detectron2.data import build_detection_train_loader\n",
    "    from detectron2.checkpoint import DetectionCheckpointer\n",
    "    from checkpointer import ValCheckpointer #eh, it gets the essence of early stopping & Im too lazy to make it actually early stop\n",
    "\n",
    "    optimizer = build_optimizer(cfg, model)\n",
    "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
    "    saver = DetectionCheckpointer(model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler)\n",
    "    meta = saver.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume) #loads the model weights & returns stored meta\n",
    "\n",
    "    val_loss = meta.get('min_val_loss',meta.get('val_loss',999))\n",
    "    checkpointer = ValCheckpointer(saver,cfg.SAVE_EVERY,model_context,lambda: do_eval(cfg,model),val_loss)\n",
    "    \n",
    "    if 'EPOCHS' in cfg.SOLVER.keys(): cfg.SOLVER.MAX_ITER = cfg.SOLVER.EPOCHS*cfg.EPOCH_SIZE\n",
    "    max_iter = cfg.SOLVER.MAX_ITER\n",
    "    start_iter = meta.get(\"iteration\",-1)+1 \n",
    "\n",
    "    if resume: \n",
    "        logger.info(f\"Resumed model: {meta.get('model_name','unknown')}\")\n",
    "        #override some configs from checkpoint in case you changed them\n",
    "        scheduler.milestones = cfg.SOLVER.STEPS\n",
    "        scheduler.gamma = cfg.SOLVER.GAMMA\n",
    "        scheduler.base_lrs = [cfg.SOLVER.BASE_LR for lr in scheduler.base_lrs]\n",
    "        scheduler.last_epoch = start_iter\n",
    "\n",
    "    writers = [\n",
    "        CommonMetricPrinter(max_iter),\n",
    "        JSONWriter(f\"{cfg.OUTPUT_DIR}/{model_context}-metrics.json\"),\n",
    "        TensorboardXWriter(cfg.OUTPUT_DIR),\n",
    "    ]\n",
    "\n",
    "    model.train() #set to training mode (PyTorch)\n",
    "    dataloader = build_detection_train_loader(cfg,mapper=DatasetPipeline(cfg,True))\n",
    "    logger.info(f\"Training: Start Iter {start_iter}, End Iter {max_iter}\")\n",
    "    with EventStorage(start_iter) as storage:\n",
    "        for iteration,data in zip(range(start_iter,max_iter),dataloader):\n",
    "            try:\n",
    "                iteration = iteration + 1\n",
    "                storage.step()\n",
    "                \n",
    "                loss_dict = model(data)\n",
    "                losses = sum(loss_dict.values())\n",
    "                assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "                loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
    "                losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "                if comm.is_main_process(): storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
    "                scheduler.step()\n",
    "\n",
    "                if iteration - start_iter > 5 and (iteration % cfg.PRINT_EVERY == 0 or iteration == max_iter):\n",
    "                    for writer in writers: writer.write()\n",
    "                checkpointer.step(iteration)\n",
    "                \n",
    "            except (Exception,KeyboardInterrupt) as e:\n",
    "                logger.info(\"ERROR! Dumping current model...\")\n",
    "                checkpointer.save(f\"{model_context}-{iteration}-interrupted\",iteration=iteration,model_name=model_context,min_val_loss=checkpointer.min_loss)\n",
    "                raise e\n",
    "    checkpointer.save(f\"{model_context}-{max_iter}-final\",iteration=max_iter,model_name=model_context,min_val_loss=checkpointer.min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".000250  max_mem: 5317M\n\u001b[32m[06/15 13:25:04 d2.utils.events]: \u001b[0m eta: 1:48:51  iter: 125800  total_loss: 0.144  loss_cls: 0.050  loss_box_reg: 0.079  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:27:00 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 13:27:00 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 13:27:00 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 13:27:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 13:27:00 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.49it/s]\n\n\u001b[32m[06/15 13:29:21 detectron2]: \u001b[0mVal Loss: 0.2837391197681427 @ Iteration 125999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 13:29:21 d2.utils.events]: \u001b[0m eta: 3:56:59  iter: 126000  total_loss: 0.112  loss_cls: 0.036  loss_box_reg: 0.068  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:31:20 d2.utils.events]: \u001b[0m eta: 1:47:16  iter: 126200  total_loss: 0.161  loss_cls: 0.052  loss_box_reg: 0.084  loss_rpn_cls: 0.002  loss_rpn_loc: 0.007  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:33:17 d2.utils.events]: \u001b[0m eta: 1:44:07  iter: 126400  total_loss: 0.132  loss_cls: 0.048  loss_box_reg: 0.074  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:35:14 d2.utils.events]: \u001b[0m eta: 1:42:09  iter: 126600  total_loss: 0.137  loss_cls: 0.047  loss_box_reg: 0.075  loss_rpn_cls: 0.000  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:37:11 d2.utils.events]: \u001b[0m eta: 1:39:58  iter: 126800  total_loss: 0.125  loss_cls: 0.042  loss_box_reg: 0.074  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:39:08 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 13:39:08 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 13:39:08 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 13:39:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 13:39:08 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.48it/s]\n\n\u001b[32m[06/15 13:41:28 detectron2]: \u001b[0mVal Loss: 0.2965153753757477 @ Iteration 126999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 13:41:29 d2.utils.events]: \u001b[0m eta: 3:35:41  iter: 127000  total_loss: 0.117  loss_cls: 0.040  loss_box_reg: 0.072  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:43:28 d2.utils.events]: \u001b[0m eta: 1:37:23  iter: 127200  total_loss: 0.108  loss_cls: 0.035  loss_box_reg: 0.067  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:45:24 d2.utils.events]: \u001b[0m eta: 1:33:26  iter: 127400  total_loss: 0.137  loss_cls: 0.050  loss_box_reg: 0.083  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:47:21 d2.utils.events]: \u001b[0m eta: 1:32:17  iter: 127600  total_loss: 0.134  loss_cls: 0.046  loss_box_reg: 0.075  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:49:20 d2.utils.events]: \u001b[0m eta: 1:31:21  iter: 127800  total_loss: 0.150  loss_cls: 0.054  loss_box_reg: 0.085  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:51:15 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 13:51:15 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 13:51:15 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 13:51:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 13:51:15 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.50it/s]\n\n\u001b[32m[06/15 13:53:36 detectron2]: \u001b[0mVal Loss: 0.2827516794204712 @ Iteration 127999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 13:53:36 d2.utils.events]: \u001b[0m eta: 3:13:33  iter: 128000  total_loss: 0.134  loss_cls: 0.044  loss_box_reg: 0.084  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:55:33 d2.utils.events]: \u001b[0m eta: 1:26:30  iter: 128200  total_loss: 0.122  loss_cls: 0.038  loss_box_reg: 0.071  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:57:31 d2.utils.events]: \u001b[0m eta: 1:24:58  iter: 128400  total_loss: 0.126  loss_cls: 0.047  loss_box_reg: 0.067  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 13:59:28 d2.utils.events]: \u001b[0m eta: 1:21:54  iter: 128600  total_loss: 0.167  loss_cls: 0.054  loss_box_reg: 0.097  loss_rpn_cls: 0.005  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:01:26 d2.utils.events]: \u001b[0m eta: 1:21:38  iter: 128800  total_loss: 0.124  loss_cls: 0.040  loss_box_reg: 0.064  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:03:23 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 14:03:23 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 14:03:23 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 14:03:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 14:03:23 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.48it/s]\n\n\u001b[32m[06/15 14:05:44 detectron2]: \u001b[0mVal Loss: 0.2903779149055481 @ Iteration 128999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 14:05:44 d2.utils.events]: \u001b[0m eta: 2:52:46  iter: 129000  total_loss: 0.111  loss_cls: 0.042  loss_box_reg: 0.063  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:07:41 d2.utils.events]: \u001b[0m eta: 1:16:36  iter: 129200  total_loss: 0.133  loss_cls: 0.042  loss_box_reg: 0.071  loss_rpn_cls: 0.002  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:09:39 d2.utils.events]: \u001b[0m eta: 1:15:05  iter: 129400  total_loss: 0.117  loss_cls: 0.036  loss_box_reg: 0.073  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:11:35 d2.utils.events]: \u001b[0m eta: 1:11:49  iter: 129600  total_loss: 0.121  loss_cls: 0.039  loss_box_reg: 0.073  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:13:32 d2.utils.events]: \u001b[0m eta: 1:11:08  iter: 129800  total_loss: 0.139  loss_cls: 0.045  loss_box_reg: 0.080  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:15:29 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 14:15:29 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 14:15:29 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 14:15:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 14:15:29 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.49it/s]\n\n\u001b[32m[06/15 14:17:50 detectron2]: \u001b[0mVal Loss: 0.2786967158317566 @ Iteration 129999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 14:17:50 d2.utils.events]: \u001b[0m eta: 2:31:26  iter: 130000  total_loss: 0.132  loss_cls: 0.039  loss_box_reg: 0.077  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:19:47 d2.utils.events]: \u001b[0m eta: 1:06:43  iter: 130200  total_loss: 0.133  loss_cls: 0.044  loss_box_reg: 0.078  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:21:43 d2.utils.events]: \u001b[0m eta: 1:04:21  iter: 130400  total_loss: 0.150  loss_cls: 0.049  loss_box_reg: 0.087  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:23:39 d2.utils.events]: \u001b[0m eta: 1:02:22  iter: 130600  total_loss: 0.127  loss_cls: 0.044  loss_box_reg: 0.075  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:25:36 d2.utils.events]: \u001b[0m eta: 1:01:00  iter: 130800  total_loss: 0.120  loss_cls: 0.041  loss_box_reg: 0.076  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:27:33 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 14:27:33 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 14:27:33 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 14:27:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 14:27:33 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.50it/s]\n\n\u001b[32m[06/15 14:29:53 detectron2]: \u001b[0mVal Loss: 0.283726304769516 @ Iteration 130999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 14:29:54 d2.utils.events]: \u001b[0m eta: 2:09:42  iter: 131000  total_loss: 0.132  loss_cls: 0.043  loss_box_reg: 0.079  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:31:49 d2.utils.events]: \u001b[0m eta: 0:56:23  iter: 131200  total_loss: 0.115  loss_cls: 0.043  loss_box_reg: 0.064  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:33:47 d2.utils.events]: \u001b[0m eta: 0:55:20  iter: 131400  total_loss: 0.129  loss_cls: 0.045  loss_box_reg: 0.065  loss_rpn_cls: 0.002  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:35:45 d2.utils.events]: \u001b[0m eta: 0:53:36  iter: 131600  total_loss: 0.151  loss_cls: 0.050  loss_box_reg: 0.080  loss_rpn_cls: 0.002  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:37:42 d2.utils.events]: \u001b[0m eta: 0:51:20  iter: 131800  total_loss: 0.114  loss_cls: 0.040  loss_box_reg: 0.068  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:39:40 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 14:39:40 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 14:39:40 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 14:39:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 14:39:40 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.50it/s]\n\n\u001b[32m[06/15 14:42:00 detectron2]: \u001b[0mVal Loss: 0.28655901551246643 @ Iteration 131999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 14:42:01 d2.utils.events]: \u001b[0m eta: 1:48:42  iter: 132000  total_loss: 0.146  loss_cls: 0.049  loss_box_reg: 0.086  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:43:58 d2.utils.events]: \u001b[0m eta: 0:47:25  iter: 132200  total_loss: 0.139  loss_cls: 0.048  loss_box_reg: 0.084  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:45:54 d2.utils.events]: \u001b[0m eta: 0:45:05  iter: 132400  total_loss: 0.150  loss_cls: 0.057  loss_box_reg: 0.078  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:47:52 d2.utils.events]: \u001b[0m eta: 0:43:49  iter: 132600  total_loss: 0.121  loss_cls: 0.044  loss_box_reg: 0.068  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:49:50 d2.utils.events]: \u001b[0m eta: 0:41:45  iter: 132800  total_loss: 0.118  loss_cls: 0.041  loss_box_reg: 0.070  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:51:47 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 14:51:47 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 14:51:47 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 14:51:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 14:51:47 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.48it/s]\n\n\u001b[32m[06/15 14:54:08 detectron2]: \u001b[0mVal Loss: 0.2791896462440491 @ Iteration 132999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 14:54:08 d2.utils.events]: \u001b[0m eta: 1:26:57  iter: 133000  total_loss: 0.136  loss_cls: 0.048  loss_box_reg: 0.082  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:56:05 d2.utils.events]: \u001b[0m eta: 0:37:36  iter: 133200  total_loss: 0.153  loss_cls: 0.050  loss_box_reg: 0.096  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:58:00 d2.utils.events]: \u001b[0m eta: 0:35:01  iter: 133400  total_loss: 0.129  loss_cls: 0.040  loss_box_reg: 0.073  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 14:59:56 d2.utils.events]: \u001b[0m eta: 0:33:17  iter: 133600  total_loss: 0.139  loss_cls: 0.045  loss_box_reg: 0.081  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:01:52 d2.utils.events]: \u001b[0m eta: 0:31:28  iter: 133800  total_loss: 0.123  loss_cls: 0.038  loss_box_reg: 0.083  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:03:49 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 15:03:49 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 15:03:49 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 15:03:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 15:03:49 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.49it/s]\n\n\u001b[32m[06/15 15:06:10 detectron2]: \u001b[0mVal Loss: 0.2909696400165558 @ Iteration 133999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 15:06:10 d2.utils.events]: \u001b[0m eta: 1:05:34  iter: 134000  total_loss: 0.147  loss_cls: 0.054  loss_box_reg: 0.084  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:08:08 d2.utils.events]: \u001b[0m eta: 0:27:55  iter: 134200  total_loss: 0.118  loss_cls: 0.038  loss_box_reg: 0.074  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:10:07 d2.utils.events]: \u001b[0m eta: 0:26:20  iter: 134400  total_loss: 0.141  loss_cls: 0.047  loss_box_reg: 0.081  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:12:04 d2.utils.events]: \u001b[0m eta: 0:23:51  iter: 134600  total_loss: 0.111  loss_cls: 0.035  loss_box_reg: 0.070  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:13:59 d2.utils.events]: \u001b[0m eta: 0:21:32  iter: 134800  total_loss: 0.125  loss_cls: 0.041  loss_box_reg: 0.071  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:15:57 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 15:15:57 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 15:15:57 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 15:15:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 15:15:57 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.50it/s]\n\n\u001b[32m[06/15 15:18:17 detectron2]: \u001b[0mVal Loss: 0.291153222322464 @ Iteration 134999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 15:18:17 d2.utils.events]: \u001b[0m eta: 0:44:09  iter: 135000  total_loss: 0.113  loss_cls: 0.042  loss_box_reg: 0.066  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:20:15 d2.utils.events]: \u001b[0m eta: 0:18:04  iter: 135200  total_loss: 0.113  loss_cls: 0.044  loss_box_reg: 0.059  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:22:12 d2.utils.events]: \u001b[0m eta: 0:16:11  iter: 135400  total_loss: 0.138  loss_cls: 0.041  loss_box_reg: 0.076  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:24:10 d2.utils.events]: \u001b[0m eta: 0:14:09  iter: 135600  total_loss: 0.118  loss_cls: 0.039  loss_box_reg: 0.073  loss_rpn_cls: 0.000  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:26:07 d2.utils.events]: \u001b[0m eta: 0:12:15  iter: 135800  total_loss: 0.115  loss_cls: 0.038  loss_box_reg: 0.065  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:28:04 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 15:28:04 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 15:28:04 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 15:28:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 15:28:04 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.50it/s]\n\n\u001b[32m[06/15 15:30:25 detectron2]: \u001b[0mVal Loss: 0.2798909842967987 @ Iteration 135999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 15:30:25 d2.utils.events]: \u001b[0m eta: 0:22:35  iter: 136000  total_loss: 0.111  loss_cls: 0.039  loss_box_reg: 0.068  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:32:22 d2.utils.events]: \u001b[0m eta: 0:08:15  iter: 136200  total_loss: 0.107  loss_cls: 0.033  loss_box_reg: 0.060  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:34:19 d2.utils.events]: \u001b[0m eta: 0:06:21  iter: 136400  total_loss: 0.142  loss_cls: 0.049  loss_box_reg: 0.087  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:36:18 d2.utils.events]: \u001b[0m eta: 0:04:26  iter: 136600  total_loss: 0.121  loss_cls: 0.046  loss_box_reg: 0.068  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:38:15 d2.utils.events]: \u001b[0m eta: 0:02:26  iter: 136800  total_loss: 0.124  loss_cls: 0.042  loss_box_reg: 0.080  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:40:14 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/15 15:40:14 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 15:40:14 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 15:40:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 15:40:14 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1474/1474 [02:20<00:00, 10.49it/s]\n\n\u001b[32m[06/15 15:42:35 detectron2]: \u001b[0mVal Loss: 0.28751108050346375 @ Iteration 136999, Min Val Loss: 0.18455912172794342, did not save model.\n\n\u001b[32m[06/15 15:42:35 d2.utils.events]: \u001b[0m eta: 0:01:05  iter: 137000  total_loss: 0.123  loss_cls: 0.043  loss_box_reg: 0.076  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  lr: 0.000250  max_mem: 5317M\n\u001b[32m[06/15 15:43:05 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 137050  total_loss: 0.149  loss_cls: 0.048  loss_box_reg: 0.083  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  lr: 0.000250  max_mem: 5317M\n"
    }
   ],
   "source": [
    "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "cfg.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn-17999-best_val.pth\")\n",
    "do_train(cfg,model,\"ft-til_resnet101_rcnn\",resume=True)\n",
    "#if this doesnt work correctly, check what model is set in the  file in /ckpts/last_checkpoint\n",
    "#I uploaded the model file to the google drive\n",
    "#have 40% of the images contain no 'person's to prevent overeagerness. figure out how to filter out 'person' images so you can delete the rest of the enormous coco dataset. \n",
    "#^use segmentation model, that way you can remove the background before passing it to the fashion model. How to indicate it is the background tho? put a checkerboard? somehow prevent person/clothes blending into background..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/15 15:47:53 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from til2020/val.json\n\u001b[32m[06/15 15:47:53 d2.data.build]: \u001b[0mDistribution of instances among all 5 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    tops    | 317          |  trousers  | 313          | outerwear  | 316          |\n|  dresses   | 1338         |   skirts   | 174          |            |              |\n|   total    | 2458         |            |              |            |              |\u001b[0m\n\u001b[32m[06/15 15:47:53 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/15 15:47:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n\u001b[32m[06/15 15:47:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 1474 images\n\u001b[32m[06/15 15:47:56 d2.evaluation.evaluator]: \u001b[0mInference done 11/1474. 0.1629 s / img. ETA=0:03:58\n\u001b[32m[06/15 15:48:01 d2.evaluation.evaluator]: \u001b[0mInference done 39/1474. 0.1781 s / img. ETA=0:04:16\n\u001b[32m[06/15 15:48:06 d2.evaluation.evaluator]: \u001b[0mInference done 67/1474. 0.1796 s / img. ETA=0:04:13\n\u001b[32m[06/15 15:48:11 d2.evaluation.evaluator]: \u001b[0mInference done 96/1474. 0.1770 s / img. ETA=0:04:05\n\u001b[32m[06/15 15:48:16 d2.evaluation.evaluator]: \u001b[0mInference done 124/1474. 0.1775 s / img. ETA=0:04:00\n\u001b[32m[06/15 15:48:21 d2.evaluation.evaluator]: \u001b[0mInference done 153/1474. 0.1776 s / img. ETA=0:03:55\n\u001b[32m[06/15 15:48:26 d2.evaluation.evaluator]: \u001b[0mInference done 180/1474. 0.1787 s / img. ETA=0:03:52\n\u001b[32m[06/15 15:48:31 d2.evaluation.evaluator]: \u001b[0mInference done 208/1474. 0.1795 s / img. ETA=0:03:48\n\u001b[32m[06/15 15:48:36 d2.evaluation.evaluator]: \u001b[0mInference done 236/1474. 0.1793 s / img. ETA=0:03:43\n\u001b[32m[06/15 15:48:41 d2.evaluation.evaluator]: \u001b[0mInference done 263/1474. 0.1799 s / img. ETA=0:03:38\n\u001b[32m[06/15 15:48:46 d2.evaluation.evaluator]: \u001b[0mInference done 292/1474. 0.1795 s / img. ETA=0:03:33\n\u001b[32m[06/15 15:48:52 d2.evaluation.evaluator]: \u001b[0mInference done 320/1474. 0.1799 s / img. ETA=0:03:28\n\u001b[32m[06/15 15:48:57 d2.evaluation.evaluator]: \u001b[0mInference done 348/1474. 0.1800 s / img. ETA=0:03:23\n\u001b[32m[06/15 15:49:02 d2.evaluation.evaluator]: \u001b[0mInference done 377/1474. 0.1798 s / img. ETA=0:03:18\n\u001b[32m[06/15 15:49:07 d2.evaluation.evaluator]: \u001b[0mInference done 406/1474. 0.1795 s / img. ETA=0:03:12\n\u001b[32m[06/15 15:49:12 d2.evaluation.evaluator]: \u001b[0mInference done 434/1474. 0.1795 s / img. ETA=0:03:07\n\u001b[32m[06/15 15:49:17 d2.evaluation.evaluator]: \u001b[0mInference done 461/1474. 0.1798 s / img. ETA=0:03:03\n\u001b[32m[06/15 15:49:22 d2.evaluation.evaluator]: \u001b[0mInference done 489/1474. 0.1798 s / img. ETA=0:02:57\n\u001b[32m[06/15 15:49:27 d2.evaluation.evaluator]: \u001b[0mInference done 517/1474. 0.1798 s / img. ETA=0:02:52\n\u001b[32m[06/15 15:49:32 d2.evaluation.evaluator]: \u001b[0mInference done 545/1474. 0.1797 s / img. ETA=0:02:47\n\u001b[32m[06/15 15:49:37 d2.evaluation.evaluator]: \u001b[0mInference done 571/1474. 0.1803 s / img. ETA=0:02:43\n\u001b[32m[06/15 15:49:42 d2.evaluation.evaluator]: \u001b[0mInference done 599/1474. 0.1802 s / img. ETA=0:02:38\n\u001b[32m[06/15 15:49:47 d2.evaluation.evaluator]: \u001b[0mInference done 630/1474. 0.1795 s / img. ETA=0:02:32\n\u001b[32m[06/15 15:49:52 d2.evaluation.evaluator]: \u001b[0mInference done 659/1474. 0.1793 s / img. ETA=0:02:26\n\u001b[32m[06/15 15:49:57 d2.evaluation.evaluator]: \u001b[0mInference done 689/1474. 0.1787 s / img. ETA=0:02:20\n\u001b[32m[06/15 15:50:03 d2.evaluation.evaluator]: \u001b[0mInference done 720/1474. 0.1780 s / img. ETA=0:02:14\n\u001b[32m[06/15 15:50:08 d2.evaluation.evaluator]: \u001b[0mInference done 765/1474. 0.1741 s / img. ETA=0:02:04\n\u001b[32m[06/15 15:50:13 d2.evaluation.evaluator]: \u001b[0mInference done 807/1474. 0.1713 s / img. ETA=0:01:54\n\u001b[32m[06/15 15:50:18 d2.evaluation.evaluator]: \u001b[0mInference done 844/1474. 0.1698 s / img. ETA=0:01:47\n\u001b[32m[06/15 15:50:23 d2.evaluation.evaluator]: \u001b[0mInference done 890/1474. 0.1666 s / img. ETA=0:01:37\n\u001b[32m[06/15 15:50:28 d2.evaluation.evaluator]: \u001b[0mInference done 930/1474. 0.1648 s / img. ETA=0:01:30\n\u001b[32m[06/15 15:50:33 d2.evaluation.evaluator]: \u001b[0mInference done 959/1474. 0.1651 s / img. ETA=0:01:25\n\u001b[32m[06/15 15:50:38 d2.evaluation.evaluator]: \u001b[0mInference done 987/1474. 0.1655 s / img. ETA=0:01:21\n\u001b[32m[06/15 15:50:43 d2.evaluation.evaluator]: \u001b[0mInference done 1024/1474. 0.1644 s / img. ETA=0:01:14\n\u001b[32m[06/15 15:50:48 d2.evaluation.evaluator]: \u001b[0mInference done 1065/1474. 0.1628 s / img. ETA=0:01:06\n\u001b[32m[06/15 15:50:53 d2.evaluation.evaluator]: \u001b[0mInference done 1114/1474. 0.1601 s / img. ETA=0:00:57\n\u001b[32m[06/15 15:50:58 d2.evaluation.evaluator]: \u001b[0mInference done 1164/1474. 0.1576 s / img. ETA=0:00:49\n\u001b[32m[06/15 15:51:03 d2.evaluation.evaluator]: \u001b[0mInference done 1201/1474. 0.1569 s / img. ETA=0:00:43\n\u001b[32m[06/15 15:51:08 d2.evaluation.evaluator]: \u001b[0mInference done 1248/1474. 0.1549 s / img. ETA=0:00:35\n\u001b[32m[06/15 15:51:13 d2.evaluation.evaluator]: \u001b[0mInference done 1289/1474. 0.1538 s / img. ETA=0:00:28\n\u001b[32m[06/15 15:51:18 d2.evaluation.evaluator]: \u001b[0mInference done 1327/1474. 0.1532 s / img. ETA=0:00:22\n\u001b[32m[06/15 15:51:23 d2.evaluation.evaluator]: \u001b[0mInference done 1375/1474. 0.1515 s / img. ETA=0:00:15\n\u001b[32m[06/15 15:51:28 d2.evaluation.evaluator]: \u001b[0mInference done 1425/1474. 0.1497 s / img. ETA=0:00:07\n\u001b[32m[06/15 15:51:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:38.879148 (0.148999 s / img per device, on 1 devices)\n\u001b[32m[06/15 15:51:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:03:37 (0.148062 s / img per device, on 1 devices)\n\u001b[32m[06/15 15:51:34 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n\u001b[32m[06/15 15:51:34 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions ...\nLoading and preparing results...\nDONE (t=0.01s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=0.95s).\nAccumulating evaluation results...\nDONE (t=0.27s).\n Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.687\n Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.701\n Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.694\n Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.686\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.665\n\u001b[32m[06/15 15:51:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n|:------:|:------:|:------:|:------:|:------:|:-----:|\n| 68.744 | 70.114 | 69.380 | 68.594 | 66.490 | 0.000 |\n\u001b[32m[06/15 15:51:35 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n| category   | AP     | category   | AP     | category   | AP     |\n|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n| tops       | 58.582 | trousers   | 49.809 | outerwear  | 78.678 |\n| dresses    | 96.898 | skirts     | 59.754 |            |        |\n"
    }
   ],
   "source": [
    "#if do_train() hasnt been called (it loads the model) and you want to load the model\n",
    "if True:\n",
    "    from detectron2.checkpoint import DetectionCheckpointer\n",
    "    cfg.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn-17999-best_val.pth\")\n",
    "    DetectionCheckpointer(model, cfg.OUTPUT_DIR).resume_or_load(cfg.MODEL.WEIGHTS, resume=False)\n",
    "\n",
    "do_test(cfg,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "cfg.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn-143999-best_val.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0 #max AP at cost of inference speed\n",
    "predictor = DefaultPredictor(cfg) #check what exactly default predictor does, and if it affects anything badly.\n",
    "#Are output boxes supposed to be rescaled? btw, the current pipeline resizes back to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes2xywh(boxes):\n",
    "    return [[x1,y1,x2-x1,y2-y1] for x1,y1,x2,y2 in boxes.tensor.tolist()]\n",
    "\n",
    "def convert_prediction(img_id,output):\n",
    "    outs = []\n",
    "    raw = output['instances']\n",
    "    boxes = boxes2xywh(raw.pred_boxes)\n",
    "    cats = raw.pred_classes.tolist()\n",
    "    scores = raw.scores.tolist()\n",
    "\n",
    "    for i in range(len(raw)):\n",
    "        outs.append({\n",
    "            'image_id':img_id,\n",
    "            'category_id':cats[i],\n",
    "            'bbox':boxes[i],\n",
    "            'score':scores[i]\n",
    "        })\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1000/1000 [02:44<00:00,  6.07it/s]\n"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "output = []\n",
    "for im_path in tqdm(list(test_imgs_folder.glob('*'))):\n",
    "    im = cv2.imread(im_path) #we are using cv2 here to be absolutely sure; this is what is in the documentation\n",
    "    output += convert_prediction(int(im_path.stem),predictor(im))\n",
    "\n",
    "with open('ans.json','w') as f:\n",
    "    json.dump(output,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "im = cv2.imread(train_imgs_folder/\"69.jpg\")\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "im_out = Image.fromarray(v.get_image()[:, :, ::-1]) #channels are reversed\n",
    "#display(im_out)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitdetectronconda1819ced9d1b04054b76de77970507a6d",
   "display_name": "Python 3.8.3 64-bit ('detectron': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}