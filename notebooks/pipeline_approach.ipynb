{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import logging\n",
    "logger = logging.getLogger('detectron2')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog,build_detection_train_loader\n",
    "from detectron2.structures import Boxes, Instances\n",
    "\n",
    "from scripts.trainer import do_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "from pathlib import Path\n",
    "base_folder = Path('..')\n",
    "data_folder = base_folder/'data'/'til2020'\n",
    "train_imgs_folder = data_folder/'train'\n",
    "train_annotations = data_folder/'train.json'\n",
    "val_imgs_folder = data_folder/'val'\n",
    "val_annotations = data_folder/'val.json'\n",
    "test_imgs_folder = data_folder/'CV_final_images'\n",
    "test_annotations = data_folder/'CV_final_evaluation.json'\n",
    "\n",
    "save_model_folder = base_folder/'ckpts'\n",
    "load_model_folder = base_folder/'final_ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"til_val\", {}, val_annotations, val_imgs_folder)\n",
    "register_coco_instances(\"til_test\", {}, test_annotations, test_imgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_cropper = get_cfg()\n",
    "cfg_cropper.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg_cropper.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "cropper = DefaultPredictor(cfg_cropper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/20 13:30:42 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from ../data/til2020/val.json\n\u001b[32m[06/20 13:30:42 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1474 images left.\n\u001b[32m[06/20 13:30:42 d2.data.build]: \u001b[0mDistribution of instances among all 5 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    tops    | 317          |  trousers  | 313          | outerwear  | 316          |\n|  dresses   | 1338         |   skirts   | 174          |            |              |\n|   total    | 2458         |            |              |            |              |\u001b[0m\n\u001b[32m[06/20 13:30:42 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/20 13:30:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.37 MiB\n\u001b[32m[06/20 13:30:42 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n\u001b[32m[06/20 13:30:42 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<detectron2.data.common.AspectRatioGroupedDataset at 0x7fce7a4d4610>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "cfg_boxer = get_cfg()\n",
    "cfg_boxer.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "\n",
    "cfg_boxer.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
    "cfg_boxer.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn_moda_od-63999-best_val.pth\")\n",
    "\n",
    "cfg_boxer.DATASETS.TRAIN = (\"til_val\",)\n",
    "cfg_boxer.DATASETS.TEST = (\"til_val\",)\n",
    "\n",
    "boxer = DefaultPredictor(cfg_boxer)\n",
    "build_detection_train_loader(cfg_boxer) #force meta to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont ever attempt to train this, it doesnt even have an training output mode let alone backprop\n",
    "class PipelineWrapper(torch.nn.Module):\n",
    "    def __init__(self,cropper,boxer):\n",
    "        super(PipelineWrapper, self).__init__()\n",
    "        self.cropper = cropper\n",
    "        self.boxer = boxer\n",
    "\n",
    "        self.crop_person_confidence = 0.95\n",
    "        self.metadata = MetadataCatalog.get(\"coco_2017_val\")\n",
    "        self.crop_boundary = 0.1\n",
    "\n",
    "    def forward(self,ims):\n",
    "        outputs = []\n",
    "        for im_data in ims:\n",
    "            #convert im back to \"normal\" (HURR DURR IM A BARBARIAN)\n",
    "            im = im_data['image'].detach().numpy().transpose(1,2,0)\n",
    "            im = cv2.resize(im,(im_data['width'],im_data['height']))\n",
    "\n",
    "            im_out = self.cropper(im)\n",
    "            im_crops = [tuple(self.crop_bbox(im,bbox)) for bbox in self.get_human_bboxes(im_out)]\n",
    "                \n",
    "            collated = []\n",
    "            for im_crop,o in im_crops:\n",
    "                inst = self.boxer(im_crop)['instances'].to('cpu')\n",
    "                boxes = (inst.pred_boxes.tensor + torch.tensor([o[0],o[1],o[0],o[1]])).tolist()\n",
    "                scores = inst.scores.tolist()\n",
    "                classes = inst.pred_classes.tolist()\n",
    "                for i in range(len(inst)): collated.append((boxes[i],scores[i],classes[i]))\n",
    "            \n",
    "            collated = sorted(collated,key=lambda x: x[1],reverse=True)[:100]\n",
    "            #print(collated)\n",
    "\n",
    "            outputs.append({\"instances\":Instances((im_data['height'],im_data['width']),\n",
    "                pred_boxes=Boxes(torch.tensor([x[0] for x in collated])),\n",
    "                scores=torch.tensor([x[1] for x in collated]),\n",
    "                pred_classes=torch.tensor([x[2] for x in collated])\n",
    "            )})\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def get_human_bboxes(self,output):\n",
    "        meta_cats = self.metadata.thing_classes\n",
    "        confidence = self.crop_person_confidence\n",
    "        raw = output['instances']\n",
    "        boxes = raw.pred_boxes.tensor.tolist()\n",
    "        cats = [meta_cats[x] for x in raw.pred_classes.tolist()]\n",
    "        scores = raw.scores.tolist()\n",
    "        return [boxes[i] for i in range(len(raw)) if cats[i] == 'person' and scores[i] >= confidence]\n",
    "\n",
    "    #works when boundary is 999 even, so it works\n",
    "    def crop_bbox(self,im,bbox):\n",
    "        b = self.crop_boundary\n",
    "        x1,y1,x2,y2 = bbox\n",
    "        h,w = im.shape[:2]\n",
    "        xf,yf = b*(x2-x1),b*(y2-y1)\n",
    "        x1,y1,x2,y2 = round(max(0,x1-xf)),round(max(0,y1-yf)),round(min(w,x2+xf)),round(min(h,y2+yf))\n",
    "        #https://github.com/yu45020/Waifu2x INSERT UPSAMPLER\n",
    "        return im[y1:y2,x1:x2],(x1,y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im = cv2.imread(\"../input.jpg\")\n",
    "tester = DefaultPredictor(cfg_boxer)\n",
    "model = PipelineWrapper(cropper,boxer)\n",
    "tester.model = model\n",
    "\n",
    "im_instance = tester(im)['instances']\n",
    "\n",
    "v = Visualizer(im, MetadataCatalog.get(\"til_val\"))\n",
    "v = v.draw_instance_predictions(im_instance)\n",
    "im_out = Image.fromarray(v.get_image()[:,:,::-1]) #channels are reversed\n",
    "display(im_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/20 13:30:49 d2.data.datasets.coco]: \u001b[0mLoaded 1972 images in COCO format from ../data/til2020/CV_final_evaluation.json\n\u001b[32m[06/20 13:30:49 d2.data.build]: \u001b[0mDistribution of instances among all 5 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    tops    | 0            |  trousers  | 0            | outerwear  | 0            |\n|  dresses   | 0            |   skirts   | 0            |            |              |\n|   total    | 0            |            |              |            |              |\u001b[0m\n\u001b[32m[06/20 13:30:49 d2.data.common]: \u001b[0mSerializing 1972 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/20 13:30:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.23 MiB\n\u001b[32m[06/20 13:30:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 1972 images\n\u001b[32m[06/20 13:30:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/1972. 0.3921 s / img. ETA=0:12:49\n\u001b[32m[06/20 13:30:59 d2.evaluation.evaluator]: \u001b[0mInference done 25/1972. 0.3787 s / img. ETA=0:12:18\n\u001b[32m[06/20 13:31:04 d2.evaluation.evaluator]: \u001b[0mInference done 41/1972. 0.3527 s / img. ETA=0:11:22\n\u001b[32m[06/20 13:31:09 d2.evaluation.evaluator]: \u001b[0mInference done 57/1972. 0.3433 s / img. ETA=0:10:58\n\u001b[32m[06/20 13:31:14 d2.evaluation.evaluator]: \u001b[0mInference done 74/1972. 0.3338 s / img. ETA=0:10:35\n\u001b[32m[06/20 13:31:20 d2.evaluation.evaluator]: \u001b[0mInference done 92/1972. 0.3248 s / img. ETA=0:10:12\n\u001b[32m[06/20 13:31:25 d2.evaluation.evaluator]: \u001b[0mInference done 109/1972. 0.3201 s / img. ETA=0:09:57\n\u001b[32m[06/20 13:31:30 d2.evaluation.evaluator]: \u001b[0mInference done 124/1972. 0.3222 s / img. ETA=0:09:56\n\u001b[32m[06/20 13:31:35 d2.evaluation.evaluator]: \u001b[0mInference done 141/1972. 0.3215 s / img. ETA=0:09:50\n\u001b[32m[06/20 13:31:40 d2.evaluation.evaluator]: \u001b[0mInference done 156/1972. 0.3238 s / img. ETA=0:09:49\n\u001b[32m[06/20 13:31:45 d2.evaluation.evaluator]: \u001b[0mInference done 172/1972. 0.3229 s / img. ETA=0:09:42\n\u001b[32m[06/20 13:31:51 d2.evaluation.evaluator]: \u001b[0mInference done 187/1972. 0.3249 s / img. ETA=0:09:41\n\u001b[32m[06/20 13:31:56 d2.evaluation.evaluator]: \u001b[0mInference done 204/1972. 0.3235 s / img. ETA=0:09:33\n\u001b[32m[06/20 13:32:01 d2.evaluation.evaluator]: \u001b[0mInference done 220/1972. 0.3236 s / img. ETA=0:09:28\n\u001b[32m[06/20 13:32:06 d2.evaluation.evaluator]: \u001b[0mInference done 236/1972. 0.3234 s / img. ETA=0:09:22\n\u001b[32m[06/20 13:32:11 d2.evaluation.evaluator]: \u001b[0mInference done 251/1972. 0.3247 s / img. ETA=0:09:20\n\u001b[32m[06/20 13:32:17 d2.evaluation.evaluator]: \u001b[0mInference done 266/1972. 0.3269 s / img. ETA=0:09:19\n\u001b[32m[06/20 13:32:22 d2.evaluation.evaluator]: \u001b[0mInference done 282/1972. 0.3265 s / img. ETA=0:09:13\n\u001b[32m[06/20 13:32:27 d2.evaluation.evaluator]: \u001b[0mInference done 299/1972. 0.3256 s / img. ETA=0:09:06\n\u001b[32m[06/20 13:32:32 d2.evaluation.evaluator]: \u001b[0mInference done 315/1972. 0.3254 s / img. ETA=0:09:00\n\u001b[32m[06/20 13:32:38 d2.evaluation.evaluator]: \u001b[0mInference done 331/1972. 0.3259 s / img. ETA=0:08:56\n\u001b[32m[06/20 13:32:43 d2.evaluation.evaluator]: \u001b[0mInference done 346/1972. 0.3262 s / img. ETA=0:08:51\n\u001b[32m[06/20 13:32:48 d2.evaluation.evaluator]: \u001b[0mInference done 364/1972. 0.3245 s / img. ETA=0:08:43\n\u001b[32m[06/20 13:32:53 d2.evaluation.evaluator]: \u001b[0mInference done 379/1972. 0.3250 s / img. ETA=0:08:39\n\u001b[32m[06/20 13:32:58 d2.evaluation.evaluator]: \u001b[0mInference done 395/1972. 0.3245 s / img. ETA=0:08:33\n\u001b[32m[06/20 13:33:04 d2.evaluation.evaluator]: \u001b[0mInference done 411/1972. 0.3248 s / img. ETA=0:08:28\n\u001b[32m[06/20 13:33:09 d2.evaluation.evaluator]: \u001b[0mInference done 426/1972. 0.3256 s / img. ETA=0:08:24\n\u001b[32m[06/20 13:33:14 d2.evaluation.evaluator]: \u001b[0mInference done 440/1972. 0.3272 s / img. ETA=0:08:22\n\u001b[32m[06/20 13:33:19 d2.evaluation.evaluator]: \u001b[0mInference done 456/1972. 0.3270 s / img. ETA=0:08:17\n\u001b[32m[06/20 13:33:24 d2.evaluation.evaluator]: \u001b[0mInference done 473/1972. 0.3259 s / img. ETA=0:08:09\n\u001b[32m[06/20 13:33:29 d2.evaluation.evaluator]: \u001b[0mInference done 490/1972. 0.3250 s / img. ETA=0:08:02\n\u001b[32m[06/20 13:33:35 d2.evaluation.evaluator]: \u001b[0mInference done 506/1972. 0.3249 s / img. ETA=0:07:57\n\u001b[32m[06/20 13:33:40 d2.evaluation.evaluator]: \u001b[0mInference done 523/1972. 0.3242 s / img. ETA=0:07:50\n\u001b[32m[06/20 13:33:45 d2.evaluation.evaluator]: \u001b[0mInference done 535/1972. 0.3263 s / img. ETA=0:07:50\n\u001b[32m[06/20 13:33:50 d2.evaluation.evaluator]: \u001b[0mInference done 552/1972. 0.3254 s / img. ETA=0:07:43\n\u001b[32m[06/20 13:33:55 d2.evaluation.evaluator]: \u001b[0mInference done 567/1972. 0.3258 s / img. ETA=0:07:38\n\u001b[32m[06/20 13:34:00 d2.evaluation.evaluator]: \u001b[0mInference done 585/1972. 0.3244 s / img. ETA=0:07:31\n\u001b[32m[06/20 13:34:05 d2.evaluation.evaluator]: \u001b[0mInference done 601/1972. 0.3242 s / img. ETA=0:07:25\n\u001b[32m[06/20 13:34:10 d2.evaluation.evaluator]: \u001b[0mInference done 616/1972. 0.3245 s / img. ETA=0:07:21\n\u001b[32m[06/20 13:34:15 d2.evaluation.evaluator]: \u001b[0mInference done 631/1972. 0.3249 s / img. ETA=0:07:16\n\u001b[32m[06/20 13:34:20 d2.evaluation.evaluator]: \u001b[0mInference done 644/1972. 0.3264 s / img. ETA=0:07:14\n\u001b[32m[06/20 13:34:26 d2.evaluation.evaluator]: \u001b[0mInference done 662/1972. 0.3254 s / img. ETA=0:07:07\n\u001b[32m[06/20 13:34:31 d2.evaluation.evaluator]: \u001b[0mInference done 679/1972. 0.3247 s / img. ETA=0:07:00\n\u001b[32m[06/20 13:34:36 d2.evaluation.evaluator]: \u001b[0mInference done 697/1972. 0.3235 s / img. ETA=0:06:53\n\u001b[32m[06/20 13:34:41 d2.evaluation.evaluator]: \u001b[0mInference done 715/1972. 0.3229 s / img. ETA=0:06:46\n\u001b[32m[06/20 13:34:46 d2.evaluation.evaluator]: \u001b[0mInference done 732/1972. 0.3224 s / img. ETA=0:06:40\n\u001b[32m[06/20 13:34:51 d2.evaluation.evaluator]: \u001b[0mInference done 747/1972. 0.3226 s / img. ETA=0:06:36\n\u001b[32m[06/20 13:34:57 d2.evaluation.evaluator]: \u001b[0mInference done 763/1972. 0.3227 s / img. ETA=0:06:31\n\u001b[32m[06/20 13:35:02 d2.evaluation.evaluator]: \u001b[0mInference done 775/1972. 0.3243 s / img. ETA=0:06:29\n\u001b[32m[06/20 13:35:07 d2.evaluation.evaluator]: \u001b[0mInference done 792/1972. 0.3243 s / img. ETA=0:06:23\n\u001b[32m[06/20 13:35:12 d2.evaluation.evaluator]: \u001b[0mInference done 807/1972. 0.3248 s / img. ETA=0:06:19\n\u001b[32m[06/20 13:35:18 d2.evaluation.evaluator]: \u001b[0mInference done 823/1972. 0.3248 s / img. ETA=0:06:14\n\u001b[32m[06/20 13:35:23 d2.evaluation.evaluator]: \u001b[0mInference done 839/1972. 0.3246 s / img. ETA=0:06:08\n\u001b[32m[06/20 13:35:28 d2.evaluation.evaluator]: \u001b[0mInference done 855/1972. 0.3244 s / img. ETA=0:06:03\n\u001b[32m[06/20 13:35:33 d2.evaluation.evaluator]: \u001b[0mInference done 870/1972. 0.3246 s / img. ETA=0:05:58\n\u001b[32m[06/20 13:35:38 d2.evaluation.evaluator]: \u001b[0mInference done 886/1972. 0.3245 s / img. ETA=0:05:53\n\u001b[32m[06/20 13:35:43 d2.evaluation.evaluator]: \u001b[0mInference done 904/1972. 0.3238 s / img. ETA=0:05:46\n\u001b[32m[06/20 13:35:48 d2.evaluation.evaluator]: \u001b[0mInference done 917/1972. 0.3250 s / img. ETA=0:05:43\n\u001b[32m[06/20 13:35:54 d2.evaluation.evaluator]: \u001b[0mInference done 936/1972. 0.3239 s / img. ETA=0:05:36\n\u001b[32m[06/20 13:35:59 d2.evaluation.evaluator]: \u001b[0mInference done 952/1972. 0.3238 s / img. ETA=0:05:31\n\u001b[32m[06/20 13:36:04 d2.evaluation.evaluator]: \u001b[0mInference done 968/1972. 0.3238 s / img. ETA=0:05:25\n\u001b[32m[06/20 13:36:09 d2.evaluation.evaluator]: \u001b[0mInference done 984/1972. 0.3237 s / img. ETA=0:05:20\n\u001b[32m[06/20 13:36:14 d2.evaluation.evaluator]: \u001b[0mInference done 997/1972. 0.3247 s / img. ETA=0:05:17\n\u001b[32m[06/20 13:36:20 d2.evaluation.evaluator]: \u001b[0mInference done 1013/1972. 0.3247 s / img. ETA=0:05:12\n\u001b[32m[06/20 13:36:25 d2.evaluation.evaluator]: \u001b[0mInference done 1027/1972. 0.3253 s / img. ETA=0:05:08\n\u001b[32m[06/20 13:36:30 d2.evaluation.evaluator]: \u001b[0mInference done 1044/1972. 0.3249 s / img. ETA=0:05:02\n\u001b[32m[06/20 13:36:35 d2.evaluation.evaluator]: \u001b[0mInference done 1062/1972. 0.3244 s / img. ETA=0:04:55\n\u001b[32m[06/20 13:36:40 d2.evaluation.evaluator]: \u001b[0mInference done 1080/1972. 0.3237 s / img. ETA=0:04:49\n\u001b[32m[06/20 13:36:45 d2.evaluation.evaluator]: \u001b[0mInference done 1096/1972. 0.3237 s / img. ETA=0:04:44\n\u001b[32m[06/20 13:36:50 d2.evaluation.evaluator]: \u001b[0mInference done 1112/1972. 0.3236 s / img. ETA=0:04:39\n\u001b[32m[06/20 13:36:56 d2.evaluation.evaluator]: \u001b[0mInference done 1131/1972. 0.3227 s / img. ETA=0:04:32\n\u001b[32m[06/20 13:37:01 d2.evaluation.evaluator]: \u001b[0mInference done 1148/1972. 0.3222 s / img. ETA=0:04:26\n\u001b[32m[06/20 13:37:06 d2.evaluation.evaluator]: \u001b[0mInference done 1166/1972. 0.3217 s / img. ETA=0:04:19\n\u001b[32m[06/20 13:37:11 d2.evaluation.evaluator]: \u001b[0mInference done 1181/1972. 0.3220 s / img. ETA=0:04:15\n\u001b[32m[06/20 13:37:16 d2.evaluation.evaluator]: \u001b[0mInference done 1198/1972. 0.3217 s / img. ETA=0:04:09\n\u001b[32m[06/20 13:37:21 d2.evaluation.evaluator]: \u001b[0mInference done 1214/1972. 0.3216 s / img. ETA=0:04:04\n\u001b[32m[06/20 13:37:26 d2.evaluation.evaluator]: \u001b[0mInference done 1231/1972. 0.3214 s / img. ETA=0:03:58\n\u001b[32m[06/20 13:37:32 d2.evaluation.evaluator]: \u001b[0mInference done 1247/1972. 0.3216 s / img. ETA=0:03:53\n\u001b[32m[06/20 13:37:37 d2.evaluation.evaluator]: \u001b[0mInference done 1266/1972. 0.3209 s / img. ETA=0:03:47\n\u001b[32m[06/20 13:37:42 d2.evaluation.evaluator]: \u001b[0mInference done 1280/1972. 0.3214 s / img. ETA=0:03:42\n\u001b[32m[06/20 13:37:47 d2.evaluation.evaluator]: \u001b[0mInference done 1296/1972. 0.3215 s / img. ETA=0:03:37\n\u001b[32m[06/20 13:37:53 d2.evaluation.evaluator]: \u001b[0mInference done 1315/1972. 0.3209 s / img. ETA=0:03:31\n\u001b[32m[06/20 13:37:58 d2.evaluation.evaluator]: \u001b[0mInference done 1332/1972. 0.3208 s / img. ETA=0:03:25\n\u001b[32m[06/20 13:38:03 d2.evaluation.evaluator]: \u001b[0mInference done 1349/1972. 0.3205 s / img. ETA=0:03:20\n\u001b[32m[06/20 13:38:08 d2.evaluation.evaluator]: \u001b[0mInference done 1368/1972. 0.3198 s / img. ETA=0:03:13\n\u001b[32m[06/20 13:38:13 d2.evaluation.evaluator]: \u001b[0mInference done 1384/1972. 0.3197 s / img. ETA=0:03:08\n\u001b[32m[06/20 13:38:19 d2.evaluation.evaluator]: \u001b[0mInference done 1399/1972. 0.3200 s / img. ETA=0:03:03\n\u001b[32m[06/20 13:38:24 d2.evaluation.evaluator]: \u001b[0mInference done 1415/1972. 0.3203 s / img. ETA=0:02:58\n\u001b[32m[06/20 13:38:29 d2.evaluation.evaluator]: \u001b[0mInference done 1431/1972. 0.3203 s / img. ETA=0:02:53\n\u001b[32m[06/20 13:38:35 d2.evaluation.evaluator]: \u001b[0mInference done 1450/1972. 0.3197 s / img. ETA=0:02:47\n\u001b[32m[06/20 13:38:40 d2.evaluation.evaluator]: \u001b[0mInference done 1467/1972. 0.3196 s / img. ETA=0:02:41\n\u001b[32m[06/20 13:38:45 d2.evaluation.evaluator]: \u001b[0mInference done 1486/1972. 0.3192 s / img. ETA=0:02:35\n\u001b[32m[06/20 13:38:50 d2.evaluation.evaluator]: \u001b[0mInference done 1502/1972. 0.3192 s / img. ETA=0:02:30\n\u001b[32m[06/20 13:38:56 d2.evaluation.evaluator]: \u001b[0mInference done 1517/1972. 0.3194 s / img. ETA=0:02:25\n\u001b[32m[06/20 13:39:01 d2.evaluation.evaluator]: \u001b[0mInference done 1533/1972. 0.3194 s / img. ETA=0:02:20\n\u001b[32m[06/20 13:39:06 d2.evaluation.evaluator]: \u001b[0mInference done 1548/1972. 0.3193 s / img. ETA=0:02:16\n\u001b[32m[06/20 13:39:11 d2.evaluation.evaluator]: \u001b[0mInference done 1559/1972. 0.3202 s / img. ETA=0:02:12\n\u001b[32m[06/20 13:39:17 d2.evaluation.evaluator]: \u001b[0mInference done 1579/1972. 0.3195 s / img. ETA=0:02:06\n\u001b[32m[06/20 13:39:22 d2.evaluation.evaluator]: \u001b[0mInference done 1596/1972. 0.3193 s / img. ETA=0:02:00\n\u001b[32m[06/20 13:39:27 d2.evaluation.evaluator]: \u001b[0mInference done 1612/1972. 0.3193 s / img. ETA=0:01:55\n\u001b[32m[06/20 13:39:32 d2.evaluation.evaluator]: \u001b[0mInference done 1629/1972. 0.3191 s / img. ETA=0:01:49\n\u001b[32m[06/20 13:39:37 d2.evaluation.evaluator]: \u001b[0mInference done 1645/1972. 0.3191 s / img. ETA=0:01:44\n\u001b[32m[06/20 13:39:42 d2.evaluation.evaluator]: \u001b[0mInference done 1662/1972. 0.3189 s / img. ETA=0:01:39\n\u001b[32m[06/20 13:39:48 d2.evaluation.evaluator]: \u001b[0mInference done 1678/1972. 0.3190 s / img. ETA=0:01:34\n\u001b[32m[06/20 13:39:53 d2.evaluation.evaluator]: \u001b[0mInference done 1694/1972. 0.3191 s / img. ETA=0:01:29\n\u001b[32m[06/20 13:39:58 d2.evaluation.evaluator]: \u001b[0mInference done 1709/1972. 0.3194 s / img. ETA=0:01:24\n\u001b[32m[06/20 13:40:03 d2.evaluation.evaluator]: \u001b[0mInference done 1727/1972. 0.3190 s / img. ETA=0:01:18\n\u001b[32m[06/20 13:40:08 d2.evaluation.evaluator]: \u001b[0mInference done 1745/1972. 0.3186 s / img. ETA=0:01:12\n\u001b[32m[06/20 13:40:14 d2.evaluation.evaluator]: \u001b[0mInference done 1762/1972. 0.3185 s / img. ETA=0:01:07\n\u001b[32m[06/20 13:40:19 d2.evaluation.evaluator]: \u001b[0mInference done 1778/1972. 0.3185 s / img. ETA=0:01:02\n\u001b[32m[06/20 13:40:24 d2.evaluation.evaluator]: \u001b[0mInference done 1793/1972. 0.3186 s / img. ETA=0:00:57\n\u001b[32m[06/20 13:40:29 d2.evaluation.evaluator]: \u001b[0mInference done 1809/1972. 0.3187 s / img. ETA=0:00:52\n\u001b[32m[06/20 13:40:34 d2.evaluation.evaluator]: \u001b[0mInference done 1825/1972. 0.3189 s / img. ETA=0:00:47\n\u001b[32m[06/20 13:40:40 d2.evaluation.evaluator]: \u001b[0mInference done 1841/1972. 0.3189 s / img. ETA=0:00:41\n\u001b[32m[06/20 13:40:45 d2.evaluation.evaluator]: \u001b[0mInference done 1858/1972. 0.3187 s / img. ETA=0:00:36\n\u001b[32m[06/20 13:40:50 d2.evaluation.evaluator]: \u001b[0mInference done 1874/1972. 0.3187 s / img. ETA=0:00:31\n\u001b[32m[06/20 13:40:55 d2.evaluation.evaluator]: \u001b[0mInference done 1892/1972. 0.3184 s / img. ETA=0:00:25\n\u001b[32m[06/20 13:41:00 d2.evaluation.evaluator]: \u001b[0mInference done 1910/1972. 0.3181 s / img. ETA=0:00:19\n\u001b[32m[06/20 13:41:05 d2.evaluation.evaluator]: \u001b[0mInference done 1929/1972. 0.3177 s / img. ETA=0:00:13\n\u001b[32m[06/20 13:41:11 d2.evaluation.evaluator]: \u001b[0mInference done 1946/1972. 0.3175 s / img. ETA=0:00:08\n\u001b[32m[06/20 13:41:16 d2.evaluation.evaluator]: \u001b[0mInference done 1962/1972. 0.3175 s / img. ETA=0:00:03\n\u001b[32m[06/20 13:41:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:10:27.468310 (0.318998 s / img per device, on 1 devices)\n\u001b[32m[06/20 13:41:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:10:24 (0.317490 s / img per device, on 1 devices)\n\u001b[32m[06/20 13:41:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n\u001b[32m[06/20 13:41:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n\u001b[32m[06/20 13:41:19 d2.evaluation.coco_evaluation]: \u001b[0mAnnotations are not available for evaluation.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<detectron2.evaluation.coco_evaluation.COCOEvaluator at 0x7fce89a55220>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = PipelineWrapper(cropper,boxer)\n",
    "do_test(cfg_boxer,model,dataset_name=\"til_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "2. Upscaling: https://github.com/yu45020/Waifu2x ?????\n",
    "4. map bbox back to original (how to increase cropping precision? upscale original image before cropping?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good reason to use R101-FPN for everything:\n",
    "1. It trains fast\n",
    "2. It uses decent vram\n",
    "3. It has near SOTA performance anyways\n",
    "4. AND IT PREDICTS FAST ANYWAYS\n",
    "5. why is this model so OP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitdetectronconda1819ced9d1b04054b76de77970507a6d",
   "display_name": "Python 3.8.3 64-bit ('detectron': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}