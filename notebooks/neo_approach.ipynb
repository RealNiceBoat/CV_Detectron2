{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import logging\n",
    "logger = logging.getLogger('detectron2')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog,build_detection_train_loader\n",
    "from detectron2.structures import Boxes, Instances\n",
    "\n",
    "from scripts.trainer import do_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "from pathlib import Path\n",
    "base_folder = Path('..')\n",
    "data_folder = base_folder/'data'/'til2020'\n",
    "train_imgs_folder = data_folder/'train'\n",
    "train_annotations = data_folder/'train.json'\n",
    "val_imgs_folder = data_folder/'val'\n",
    "val_annotations = data_folder/'val.json'\n",
    "test_imgs_folder = data_folder/'CV_interim_images'\n",
    "test_annotations = data_folder/'CV_interim_evaluation.json'\n",
    "\n",
    "save_model_folder = base_folder/'ckpts'\n",
    "load_model_folder = base_folder/'final_ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"til_val\", {}, val_annotations, val_imgs_folder)\n",
    "register_coco_instances(\"til_test\", {}, test_annotations, test_imgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_cropper = get_cfg()\n",
    "cfg_cropper.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg_cropper.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "cropper = DefaultPredictor(cfg_cropper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/19 18:22:56 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from ../data/til2020/val.json\n\u001b[32m[06/19 18:22:56 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1474 images left.\n\u001b[32m[06/19 18:22:56 d2.data.build]: \u001b[0mDistribution of instances among all 5 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    tops    | 317          |  trousers  | 313          | outerwear  | 316          |\n|  dresses   | 1338         |   skirts   | 174          |            |              |\n|   total    | 2458         |            |              |            |              |\u001b[0m\n\u001b[32m[06/19 18:22:56 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/19 18:22:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.37 MiB\n\u001b[32m[06/19 18:22:56 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n\u001b[32m[06/19 18:22:56 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<detectron2.data.common.AspectRatioGroupedDataset at 0x7f92b67ea640>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "cfg_boxer = get_cfg()\n",
    "cfg_boxer.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "\n",
    "cfg_boxer.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
    "cfg_boxer.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn-17999-best_val.pth\")\n",
    "\n",
    "cfg_boxer.DATASETS.TRAIN = (\"til_val\",)\n",
    "cfg_boxer.DATASETS.TEST = (\"til_val\",)\n",
    "\n",
    "boxer = DefaultPredictor(cfg_boxer)\n",
    "build_detection_train_loader(cfg_boxer) #force meta to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont ever attempt to train this, it doesnt even have an training output mode let alone backprop\n",
    "class PipelineWrapper(torch.nn.Module):\n",
    "    def __init__(self,cropper,boxer):\n",
    "        super(PipelineWrapper, self).__init__()\n",
    "        self.cropper = cropper\n",
    "        self.boxer = boxer\n",
    "\n",
    "        self.crop_person_confidence = 0.95\n",
    "        self.metadata = MetadataCatalog.get(\"coco_2017_val\")\n",
    "        self.crop_boundary = 0.1\n",
    "\n",
    "    def forward(self,ims):\n",
    "        outputs = []\n",
    "        for im_data in ims:\n",
    "            #convert im back to \"normal\" (HURR DURR IM A BARBARIAN)\n",
    "            im = im_data['image'].detach().numpy().transpose(1,2,0)\n",
    "            im = cv2.resize(im,(im_data['width'],im_data['height']))\n",
    "\n",
    "            im_out = self.cropper(im)\n",
    "            im_crops = [tuple(self.crop_bbox(im,bbox)) for bbox in self.get_human_bboxes(im_out)]\n",
    "                \n",
    "            collated = []\n",
    "            for im_crop,o in im_crops:\n",
    "                inst = self.boxer(im_crop)['instances'].to('cpu')\n",
    "                boxes = (inst.pred_boxes.tensor + torch.tensor([o[0],o[1],o[0],o[1]])).tolist()\n",
    "                scores = inst.scores.tolist()\n",
    "                classes = inst.pred_classes.tolist()\n",
    "                for i in range(len(inst)): collated.append((boxes[i],scores[i],classes[i]))\n",
    "            \n",
    "            collated = sorted(collated,key=lambda x: x[1],reverse=True)[:100]\n",
    "            #print(collated)\n",
    "\n",
    "            outputs.append({\"instances\":Instances((im_data['height'],im_data['width']),\n",
    "                pred_boxes=Boxes(torch.tensor([x[0] for x in collated])),\n",
    "                scores=torch.tensor([x[1] for x in collated]),\n",
    "                pred_classes=torch.tensor([x[2] for x in collated])\n",
    "            )})\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def get_human_bboxes(self,output):\n",
    "        meta_cats = self.metadata.thing_classes\n",
    "        confidence = self.crop_person_confidence\n",
    "        raw = output['instances']\n",
    "        boxes = raw.pred_boxes.tensor.tolist()\n",
    "        cats = [meta_cats[x] for x in raw.pred_classes.tolist()]\n",
    "        scores = raw.scores.tolist()\n",
    "        return [boxes[i] for i in range(len(raw)) if cats[i] == 'person' and scores[i] >= confidence]\n",
    "\n",
    "    #works when boundary is 999 even, so it works\n",
    "    def crop_bbox(self,im,bbox):\n",
    "        b = self.crop_boundary\n",
    "        x1,y1,x2,y2 = bbox\n",
    "        h,w = im.shape[:2]\n",
    "        xf,yf = b*(x2-x1),b*(y2-y1)\n",
    "        x1,y1,x2,y2 = round(max(0,x1-xf)),round(max(0,y1-yf)),round(min(w,x2+xf)),round(min(h,y2+yf))\n",
    "        #https://github.com/yu45020/Waifu2x INSERT UPSAMPLER\n",
    "        return im[y1:y2,x1:x2],(x1,y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im = cv2.imread(\"../input.jpg\")\n",
    "tester = DefaultPredictor(cfg_boxer)\n",
    "model = PipelineWrapper(cropper,boxer)\n",
    "tester.model = model\n",
    "\n",
    "im_instance = tester(im)['instances']\n",
    "\n",
    "v = Visualizer(im, MetadataCatalog.get(\"til_val\"))\n",
    "v = v.draw_instance_predictions(im_instance)\n",
    "im_out = Image.fromarray(v.get_image()[:,:,::-1]) #channels are reversed\n",
    "display(im_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/19 18:23:02 d2.data.datasets.coco]: \u001b[0mLoaded 1000 images in COCO format from ../data/til2020/CV_interim_evaluation.json\n\u001b[32m[06/19 18:23:02 d2.data.build]: \u001b[0mDistribution of instances among all 5 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    tops    | 0            |  trousers  | 0            | outerwear  | 0            |\n|  dresses   | 0            |   skirts   | 0            |            |              |\n|   total    | 0            |            |              |            |              |\u001b[0m\n\u001b[32m[06/19 18:23:02 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/19 18:23:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.12 MiB\n\u001b[32m[06/19 18:23:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 1000 images\n\u001b[32m[06/19 18:23:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/1000. 0.3102 s / img. ETA=0:05:07\n\u001b[32m[06/19 18:23:11 d2.evaluation.evaluator]: \u001b[0mInference done 27/1000. 0.3268 s / img. ETA=0:05:18\n\u001b[32m[06/19 18:23:16 d2.evaluation.evaluator]: \u001b[0mInference done 44/1000. 0.3178 s / img. ETA=0:05:04\n\u001b[32m[06/19 18:23:21 d2.evaluation.evaluator]: \u001b[0mInference done 63/1000. 0.3027 s / img. ETA=0:04:44\n\u001b[32m[06/19 18:23:26 d2.evaluation.evaluator]: \u001b[0mInference done 80/1000. 0.3012 s / img. ETA=0:04:37\n\u001b[32m[06/19 18:23:31 d2.evaluation.evaluator]: \u001b[0mInference done 97/1000. 0.3013 s / img. ETA=0:04:32\n\u001b[32m[06/19 18:23:36 d2.evaluation.evaluator]: \u001b[0mInference done 114/1000. 0.3009 s / img. ETA=0:04:27\n\u001b[32m[06/19 18:23:42 d2.evaluation.evaluator]: \u001b[0mInference done 129/1000. 0.3088 s / img. ETA=0:04:29\n\u001b[32m[06/19 18:23:47 d2.evaluation.evaluator]: \u001b[0mInference done 144/1000. 0.3124 s / img. ETA=0:04:28\n\u001b[32m[06/19 18:23:52 d2.evaluation.evaluator]: \u001b[0mInference done 159/1000. 0.3151 s / img. ETA=0:04:25\n\u001b[32m[06/19 18:23:57 d2.evaluation.evaluator]: \u001b[0mInference done 175/1000. 0.3151 s / img. ETA=0:04:20\n\u001b[32m[06/19 18:24:02 d2.evaluation.evaluator]: \u001b[0mInference done 193/1000. 0.3120 s / img. ETA=0:04:12\n\u001b[32m[06/19 18:24:07 d2.evaluation.evaluator]: \u001b[0mInference done 210/1000. 0.3111 s / img. ETA=0:04:06\n\u001b[32m[06/19 18:24:13 d2.evaluation.evaluator]: \u001b[0mInference done 226/1000. 0.3123 s / img. ETA=0:04:02\n\u001b[32m[06/19 18:24:18 d2.evaluation.evaluator]: \u001b[0mInference done 243/1000. 0.3115 s / img. ETA=0:03:56\n\u001b[32m[06/19 18:24:23 d2.evaluation.evaluator]: \u001b[0mInference done 260/1000. 0.3118 s / img. ETA=0:03:51\n\u001b[32m[06/19 18:24:28 d2.evaluation.evaluator]: \u001b[0mInference done 278/1000. 0.3110 s / img. ETA=0:03:45\n\u001b[32m[06/19 18:24:33 d2.evaluation.evaluator]: \u001b[0mInference done 294/1000. 0.3111 s / img. ETA=0:03:40\n\u001b[32m[06/19 18:24:39 d2.evaluation.evaluator]: \u001b[0mInference done 312/1000. 0.3102 s / img. ETA=0:03:34\n\u001b[32m[06/19 18:24:44 d2.evaluation.evaluator]: \u001b[0mInference done 327/1000. 0.3116 s / img. ETA=0:03:30\n\u001b[32m[06/19 18:24:49 d2.evaluation.evaluator]: \u001b[0mInference done 342/1000. 0.3136 s / img. ETA=0:03:26\n\u001b[32m[06/19 18:24:54 d2.evaluation.evaluator]: \u001b[0mInference done 357/1000. 0.3145 s / img. ETA=0:03:22\n\u001b[32m[06/19 18:24:59 d2.evaluation.evaluator]: \u001b[0mInference done 375/1000. 0.3130 s / img. ETA=0:03:16\n\u001b[32m[06/19 18:25:05 d2.evaluation.evaluator]: \u001b[0mInference done 391/1000. 0.3134 s / img. ETA=0:03:11\n\u001b[32m[06/19 18:25:10 d2.evaluation.evaluator]: \u001b[0mInference done 406/1000. 0.3142 s / img. ETA=0:03:07\n\u001b[32m[06/19 18:25:15 d2.evaluation.evaluator]: \u001b[0mInference done 421/1000. 0.3155 s / img. ETA=0:03:03\n\u001b[32m[06/19 18:25:20 d2.evaluation.evaluator]: \u001b[0mInference done 437/1000. 0.3160 s / img. ETA=0:02:58\n\u001b[32m[06/19 18:25:25 d2.evaluation.evaluator]: \u001b[0mInference done 451/1000. 0.3173 s / img. ETA=0:02:54\n\u001b[32m[06/19 18:25:30 d2.evaluation.evaluator]: \u001b[0mInference done 468/1000. 0.3170 s / img. ETA=0:02:49\n\u001b[32m[06/19 18:25:36 d2.evaluation.evaluator]: \u001b[0mInference done 486/1000. 0.3156 s / img. ETA=0:02:42\n\u001b[32m[06/19 18:25:41 d2.evaluation.evaluator]: \u001b[0mInference done 503/1000. 0.3149 s / img. ETA=0:02:36\n\u001b[32m[06/19 18:25:46 d2.evaluation.evaluator]: \u001b[0mInference done 519/1000. 0.3155 s / img. ETA=0:02:32\n\u001b[32m[06/19 18:25:51 d2.evaluation.evaluator]: \u001b[0mInference done 536/1000. 0.3151 s / img. ETA=0:02:26\n\u001b[32m[06/19 18:25:56 d2.evaluation.evaluator]: \u001b[0mInference done 553/1000. 0.3148 s / img. ETA=0:02:21\n\u001b[32m[06/19 18:26:01 d2.evaluation.evaluator]: \u001b[0mInference done 572/1000. 0.3131 s / img. ETA=0:02:14\n\u001b[32m[06/19 18:26:07 d2.evaluation.evaluator]: \u001b[0mInference done 587/1000. 0.3141 s / img. ETA=0:02:10\n\u001b[32m[06/19 18:26:12 d2.evaluation.evaluator]: \u001b[0mInference done 603/1000. 0.3142 s / img. ETA=0:02:05\n\u001b[32m[06/19 18:26:17 d2.evaluation.evaluator]: \u001b[0mInference done 619/1000. 0.3146 s / img. ETA=0:02:00\n\u001b[32m[06/19 18:26:22 d2.evaluation.evaluator]: \u001b[0mInference done 634/1000. 0.3152 s / img. ETA=0:01:55\n\u001b[32m[06/19 18:26:27 d2.evaluation.evaluator]: \u001b[0mInference done 650/1000. 0.3153 s / img. ETA=0:01:50\n\u001b[32m[06/19 18:26:32 d2.evaluation.evaluator]: \u001b[0mInference done 666/1000. 0.3154 s / img. ETA=0:01:45\n\u001b[32m[06/19 18:26:37 d2.evaluation.evaluator]: \u001b[0mInference done 682/1000. 0.3153 s / img. ETA=0:01:40\n\u001b[32m[06/19 18:26:43 d2.evaluation.evaluator]: \u001b[0mInference done 697/1000. 0.3161 s / img. ETA=0:01:36\n\u001b[32m[06/19 18:26:48 d2.evaluation.evaluator]: \u001b[0mInference done 713/1000. 0.3161 s / img. ETA=0:01:30\n\u001b[32m[06/19 18:26:53 d2.evaluation.evaluator]: \u001b[0mInference done 728/1000. 0.3166 s / img. ETA=0:01:26\n\u001b[32m[06/19 18:26:58 d2.evaluation.evaluator]: \u001b[0mInference done 744/1000. 0.3169 s / img. ETA=0:01:21\n\u001b[32m[06/19 18:27:04 d2.evaluation.evaluator]: \u001b[0mInference done 762/1000. 0.3165 s / img. ETA=0:01:15\n\u001b[32m[06/19 18:27:09 d2.evaluation.evaluator]: \u001b[0mInference done 778/1000. 0.3166 s / img. ETA=0:01:10\n\u001b[32m[06/19 18:27:14 d2.evaluation.evaluator]: \u001b[0mInference done 794/1000. 0.3168 s / img. ETA=0:01:05\n\u001b[32m[06/19 18:27:19 d2.evaluation.evaluator]: \u001b[0mInference done 811/1000. 0.3167 s / img. ETA=0:01:00\n\u001b[32m[06/19 18:27:24 d2.evaluation.evaluator]: \u001b[0mInference done 830/1000. 0.3154 s / img. ETA=0:00:53\n\u001b[32m[06/19 18:27:29 d2.evaluation.evaluator]: \u001b[0mInference done 849/1000. 0.3144 s / img. ETA=0:00:47\n\u001b[32m[06/19 18:27:34 d2.evaluation.evaluator]: \u001b[0mInference done 865/1000. 0.3143 s / img. ETA=0:00:42\n\u001b[32m[06/19 18:27:40 d2.evaluation.evaluator]: \u001b[0mInference done 884/1000. 0.3141 s / img. ETA=0:00:36\n\u001b[32m[06/19 18:27:46 d2.evaluation.evaluator]: \u001b[0mInference done 900/1000. 0.3143 s / img. ETA=0:00:31\n\u001b[32m[06/19 18:27:51 d2.evaluation.evaluator]: \u001b[0mInference done 916/1000. 0.3145 s / img. ETA=0:00:26\n\u001b[32m[06/19 18:27:56 d2.evaluation.evaluator]: \u001b[0mInference done 934/1000. 0.3138 s / img. ETA=0:00:20\n\u001b[32m[06/19 18:28:01 d2.evaluation.evaluator]: \u001b[0mInference done 949/1000. 0.3141 s / img. ETA=0:00:16\n\u001b[32m[06/19 18:28:06 d2.evaluation.evaluator]: \u001b[0mInference done 966/1000. 0.3135 s / img. ETA=0:00:10\n\u001b[32m[06/19 18:28:11 d2.evaluation.evaluator]: \u001b[0mInference done 981/1000. 0.3140 s / img. ETA=0:00:05\n\u001b[32m[06/19 18:28:16 d2.evaluation.evaluator]: \u001b[0mInference done 998/1000. 0.3137 s / img. ETA=0:00:00\n\u001b[32m[06/19 18:28:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:05:13.459794 (0.315035 s / img per device, on 1 devices)\n\u001b[32m[06/19 18:28:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:11 (0.313556 s / img per device, on 1 devices)\n\u001b[32m[06/19 18:28:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n\u001b[32m[06/19 18:28:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n\u001b[32m[06/19 18:28:17 d2.evaluation.coco_evaluation]: \u001b[0mAnnotations are not available for evaluation.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<detectron2.evaluation.coco_evaluation.COCOEvaluator at 0x7f92c5d69580>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = PipelineWrapper(cropper,boxer)\n",
    "do_test(cfg_boxer,model,dataset_name=\"til_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "2. Upscaling: https://github.com/yu45020/Waifu2x ?????\n",
    "4. map bbox back to original (how to increase cropping precision? upscale original image before cropping?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good reason to use R101-FPN for everything:\n",
    "1. It trains fast\n",
    "2. It uses decent vram\n",
    "3. It has near SOTA performance anyways\n",
    "4. AND IT PREDICTS FAST ANYWAYS\n",
    "5. why is this model so OP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitdetectronconda1819ced9d1b04054b76de77970507a6d",
   "display_name": "Python 3.8.3 64-bit ('detectron': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}