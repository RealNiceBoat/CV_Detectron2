{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://detectron2.readthedocs.io/\n",
    "import detectron2\n",
    "import logging\n",
    "import torch\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "logger = logging.getLogger('detectron2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "from pathlib import Path\n",
    "base_folder = Path('..')\n",
    "data_folder = base_folder/'modanet'\n",
    "train_imgs_folder = data_folder/'train'\n",
    "train_annotations = data_folder/'ntrain.json'\n",
    "val_imgs_folder = data_folder/'train'\n",
    "val_annotations = data_folder/'nval.json'\n",
    "#test_imgs_folder = data_folder/'CV_interim_images'\n",
    "\n",
    "#keep these the same\n",
    "save_model_folder = base_folder/'ckpts'\n",
    "load_model_folder = base_folder/'ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register datasets. now they can be used as if they were native. remember to run fix_annotations.py to convert TIL2020 to proper COCO format\n",
    "#to implement custom loaders, such as pickled, https://detectron2.readthedocs.io/modules/data.html?highlight=DatasetCatalog#detectron2.data.DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"moda_train\", {}, train_annotations, train_imgs_folder)\n",
    "register_coco_instances(\"moda_val\", {}, val_annotations, val_imgs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/16 16:24:07 detectron2]: \u001b[0mLoaded Config:\nCUDNN_BENCHMARK: false\nDATALOADER:\n  ASPECT_RATIO_GROUPING: true\n  FILTER_EMPTY_ANNOTATIONS: true\n  NUM_WORKERS: 12\n  REPEAT_THRESHOLD: 0.0\n  SAMPLER_TRAIN: TrainingSampler\nDATASETS:\n  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n  PROPOSAL_FILES_TEST: []\n  PROPOSAL_FILES_TRAIN: []\n  TEST:\n  - moda_val\n  TRAIN:\n  - moda_train\nGLOBAL:\n  HACK: 1.0\nINPUT:\n  CROP:\n    ENABLED: false\n    SIZE:\n    - 0.9\n    - 0.9\n    TYPE: relative_range\n  FORMAT: BGR\n  MASK_FORMAT: polygon\n  MAX_SIZE_TEST: 1333\n  MAX_SIZE_TRAIN: 1333\n  MIN_SIZE_TEST: 800\n  MIN_SIZE_TRAIN:\n  - 640\n  - 672\n  - 704\n  - 736\n  - 768\n  - 800\n  MIN_SIZE_TRAIN_SAMPLING: choice\nMODEL:\n  ANCHOR_GENERATOR:\n    ANGLES:\n    - - -90\n      - 0\n      - 90\n    ASPECT_RATIOS:\n    - - 0.5\n      - 1.0\n      - 2.0\n    NAME: DefaultAnchorGenerator\n    OFFSET: 0.0\n    SIZES:\n    - - 32\n    - - 64\n    - - 128\n    - - 256\n    - - 512\n  BACKBONE:\n    FREEZE_AT: 2\n    NAME: build_resnet_fpn_backbone\n  DEVICE: cuda\n  FPN:\n    FUSE_TYPE: sum\n    IN_FEATURES:\n    - res2\n    - res3\n    - res4\n    - res5\n    NORM: ''\n    OUT_CHANNELS: 256\n  KEYPOINT_ON: false\n  LOAD_PROPOSALS: false\n  MASK_ON: false\n  META_ARCHITECTURE: GeneralizedRCNN\n  PANOPTIC_FPN:\n    COMBINE:\n      ENABLED: true\n      INSTANCES_CONFIDENCE_THRESH: 0.5\n      OVERLAP_THRESH: 0.5\n      STUFF_AREA_LIMIT: 4096\n    INSTANCE_LOSS_WEIGHT: 1.0\n  PIXEL_MEAN:\n  - 103.53\n  - 116.28\n  - 123.675\n  PIXEL_STD:\n  - 1.0\n  - 1.0\n  - 1.0\n  PROPOSAL_GENERATOR:\n    MIN_SIZE: 0\n    NAME: RPN\n  RESNETS:\n    DEFORM_MODULATED: false\n    DEFORM_NUM_GROUPS: 1\n    DEFORM_ON_PER_STAGE:\n    - false\n    - false\n    - false\n    - false\n    DEPTH: 101\n    NORM: FrozenBN\n    NUM_GROUPS: 1\n    OUT_FEATURES:\n    - res2\n    - res3\n    - res4\n    - res5\n    RES2_OUT_CHANNELS: 256\n    RES5_DILATION: 1\n    STEM_OUT_CHANNELS: 64\n    STRIDE_IN_1X1: true\n    WIDTH_PER_GROUP: 64\n  RETINANET:\n    BBOX_REG_WEIGHTS: &id002\n    - 1.0\n    - 1.0\n    - 1.0\n    - 1.0\n    FOCAL_LOSS_ALPHA: 0.25\n    FOCAL_LOSS_GAMMA: 2.0\n    IN_FEATURES:\n    - p3\n    - p4\n    - p5\n    - p6\n    - p7\n    IOU_LABELS:\n    - 0\n    - -1\n    - 1\n    IOU_THRESHOLDS:\n    - 0.4\n    - 0.5\n    NMS_THRESH_TEST: 0.5\n    NUM_CLASSES: 80\n    NUM_CONVS: 4\n    PRIOR_PROB: 0.01\n    SCORE_THRESH_TEST: 0.05\n    SMOOTH_L1_LOSS_BETA: 0.1\n    TOPK_CANDIDATES_TEST: 1000\n  ROI_BOX_CASCADE_HEAD:\n    BBOX_REG_WEIGHTS:\n    - &id001\n      - 10.0\n      - 10.0\n      - 5.0\n      - 5.0\n    - - 20.0\n      - 20.0\n      - 10.0\n      - 10.0\n    - - 30.0\n      - 30.0\n      - 15.0\n      - 15.0\n    IOUS:\n    - 0.5\n    - 0.6\n    - 0.7\n  ROI_BOX_HEAD:\n    BBOX_REG_WEIGHTS: *id001\n    CLS_AGNOSTIC_BBOX_REG: false\n    CONV_DIM: 256\n    FC_DIM: 1024\n    NAME: FastRCNNConvFCHead\n    NORM: ''\n    NUM_CONV: 0\n    NUM_FC: 2\n    POOLER_RESOLUTION: 7\n    POOLER_SAMPLING_RATIO: 0\n    POOLER_TYPE: ROIAlignV2\n    SMOOTH_L1_BETA: 0.0\n    TRAIN_ON_PRED_BOXES: false\n  ROI_HEADS:\n    BATCH_SIZE_PER_IMAGE: 512\n    IN_FEATURES:\n    - p2\n    - p3\n    - p4\n    - p5\n    IOU_LABELS:\n    - 0\n    - 1\n    IOU_THRESHOLDS:\n    - 0.5\n    NAME: StandardROIHeads\n    NMS_THRESH_TEST: 0.5\n    NUM_CLASSES: 13\n    POSITIVE_FRACTION: 0.25\n    PROPOSAL_APPEND_GT: true\n    SCORE_THRESH_TEST: 0.05\n  ROI_KEYPOINT_HEAD:\n    CONV_DIMS:\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    - 512\n    LOSS_WEIGHT: 1.0\n    MIN_KEYPOINTS_PER_IMAGE: 1\n    NAME: KRCNNConvDeconvUpsampleHead\n    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n    NUM_KEYPOINTS: 17\n    POOLER_RESOLUTION: 14\n    POOLER_SAMPLING_RATIO: 0\n    POOLER_TYPE: ROIAlignV2\n  ROI_MASK_HEAD:\n    CLS_AGNOSTIC_MASK: false\n    CONV_DIM: 256\n    NAME: MaskRCNNConvUpsampleHead\n    NORM: ''\n    NUM_CONV: 4\n    POOLER_RESOLUTION: 14\n    POOLER_SAMPLING_RATIO: 0\n    POOLER_TYPE: ROIAlignV2\n  RPN:\n    BATCH_SIZE_PER_IMAGE: 256\n    BBOX_REG_WEIGHTS: *id002\n    BOUNDARY_THRESH: -1\n    HEAD_NAME: StandardRPNHead\n    IN_FEATURES:\n    - p2\n    - p3\n    - p4\n    - p5\n    - p6\n    IOU_LABELS:\n    - 0\n    - -1\n    - 1\n    IOU_THRESHOLDS:\n    - 0.3\n    - 0.7\n    LOSS_WEIGHT: 1.0\n    NMS_THRESH: 0.7\n    POSITIVE_FRACTION: 0.5\n    POST_NMS_TOPK_TEST: 1000\n    POST_NMS_TOPK_TRAIN: 1000\n    PRE_NMS_TOPK_TEST: 1000\n    PRE_NMS_TOPK_TRAIN: 2000\n    SMOOTH_L1_BETA: 0.0\n  SEM_SEG_HEAD:\n    COMMON_STRIDE: 4\n    CONVS_DIM: 128\n    IGNORE_VALUE: 255\n    IN_FEATURES:\n    - p2\n    - p3\n    - p4\n    - p5\n    LOSS_WEIGHT: 1.0\n    NAME: SemSegFPNHead\n    NORM: GN\n    NUM_CLASSES: 54\n  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-101.pkl\nOUTPUT_DIR: ckpts\nPRINT_EVERY: 500\nSAVE_EVERY: 2000\nSEED: -1\nSOLVER:\n  BASE_LR: 0.000375\n  BIAS_LR_FACTOR: 1.0\n  CHECKPOINT_PERIOD: 5000\n  CLIP_GRADIENTS:\n    CLIP_TYPE: value\n    CLIP_VALUE: 1.0\n    ENABLED: false\n    NORM_TYPE: 2.0\n  GAMMA: 0.5\n  IMS_PER_BATCH: 3\n  LR_SCHEDULER_NAME: WarmupMultiStepLR\n  MAX_ITER: 1000000\n  MOMENTUM: 0.9\n  NESTEROV: false\n  STEPS:\n  - 100800000\n  WARMUP_FACTOR: 0.001\n  WARMUP_ITERS: 1000\n  WARMUP_METHOD: linear\n  WEIGHT_DECAY: 0.0001\n  WEIGHT_DECAY_BIAS: 0.0001\n  WEIGHT_DECAY_NORM: 0.0\nTEST:\n  AUG:\n    ENABLED: false\n    FLIP: true\n    MAX_SIZE: 4000\n    MIN_SIZES:\n    - 400\n    - 500\n    - 600\n    - 700\n    - 800\n    - 900\n    - 1000\n    - 1100\n    - 1200\n  DETECTIONS_PER_IMAGE: 100\n  EVAL_PERIOD: 0\n  EXPECTED_RESULTS: []\n  KEYPOINT_OKS_SIGMAS: []\n  PRECISE_BN:\n    ENABLED: false\n    NUM_ITER: 200\nVERSION: 2\nVIS_PERIOD: 0\n\n"
    }
   ],
   "source": [
    "#https://detectron2.readthedocs.io/modules/config.html\n",
    "#btw, i added some custom config options for my checkpointer and pipeline\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "#cfg.SEED = 42\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"moda_train\",)\n",
    "cfg.DATASETS.TEST = (\"moda_val\",)\n",
    "cfg.OUTPUT_DIR = str(save_model_folder)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 12\n",
    "cfg.DATALOADER.SAMPLER_TRAIN = \"TrainingSampler\" #deals with class imbalance\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 3 #batch_size\n",
    "cfg.SOLVER.BASE_LR = 0.000375\n",
    "cfg.SOLVER.GAMMA = 0.5 #halves the learning rate at each milestone\n",
    "cfg.SOLVER.STEPS = (100800000,)#(20000,60000,100000,140000) # milestones in iterations\n",
    "#^I played with these but realised it had little effect: so keep BASE_LR at 0.000125*IMS_PER_BATCH\n",
    "\n",
    "#Pipeline augmentation settings (i implemented these)\n",
    "cfg.INPUT.CROP.ENABLED = False\n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13 #number of categories\n",
    "cfg.PRINT_EVERY = 500\n",
    "cfg.SAVE_EVERY = 2000 #when using ValCheckpointer, it saves only if val loss is the minimum so far\n",
    "\n",
    "cfg.SOLVER.MAX_ITER = 1000000\n",
    "\n",
    "logger.info(f\"Loaded Config:\\n{cfg.dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (6): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (7): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (8): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (9): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (10): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (11): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (12): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (13): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (14): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (15): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (16): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (17): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (18): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (19): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (20): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (21): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (22): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=14, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=52, bias=True)\n    )\n  )\n)\n"
    }
   ],
   "source": [
    "from detectron2.modeling import build_model\n",
    "model = build_model(cfg)\n",
    "logger.info(f\"Created Model:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import DatasetPipeline\n",
    "\n",
    "#uses whatever pycocotools is installed, make sure it is the TIL one\n",
    "def do_test(cfg,model):\n",
    "    from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "    from detectron2.data import build_detection_test_loader\n",
    "    evaluator = COCOEvaluator(cfg.DATASETS.TEST[0],cfg,False)\n",
    "    val_loader = build_detection_test_loader(cfg,cfg.DATASETS.TEST[0])\n",
    "    inference_on_dataset(model,val_loader,evaluator)\n",
    "\n",
    "def do_eval(cfg,model):\n",
    "    from detectron2.data import build_detection_test_loader\n",
    "    from tqdm import tqdm\n",
    "    dataloader = build_detection_test_loader(cfg,cfg.DATASETS.TEST[0],mapper=DatasetPipeline(cfg,False))\n",
    "    total_loss = 0\n",
    "    logger.info(\"Calculating Validation Loss...\")\n",
    "    with torch.no_grad():\n",
    "        for iteration,data in enumerate(tqdm(dataloader)):\n",
    "            loss_dict = model(data)\n",
    "            total_loss += sum(loss_dict.values())\n",
    "    tqdm.write(\"\\n\")\n",
    "    return total_loss/len(dataloader)\n",
    "\n",
    "def do_train(cfg,model,model_context,resume=False):\n",
    "    from detectron2.utils.events import (\n",
    "        CommonMetricPrinter,\n",
    "        EventStorage,\n",
    "        JSONWriter,\n",
    "        TensorboardXWriter,\n",
    "    )\n",
    "    import detectron2.utils.comm as comm\n",
    "    from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "    from detectron2.data import build_detection_train_loader\n",
    "    from detectron2.checkpoint import DetectionCheckpointer\n",
    "    from checkpointer import ValCheckpointer #eh, it gets the essence of early stopping & Im too lazy to make it actually early stop\n",
    "\n",
    "    optimizer = build_optimizer(cfg, model)\n",
    "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
    "    saver = DetectionCheckpointer(model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler)\n",
    "    meta = saver.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume) #loads the model weights & returns stored meta\n",
    "\n",
    "    val_loss = meta.get('min_val_loss',meta.get('val_loss',999))\n",
    "    checkpointer = ValCheckpointer(saver,cfg.SAVE_EVERY,model_context,lambda: do_eval(cfg,model),val_loss)\n",
    "    \n",
    "    if 'EPOCHS' in cfg.SOLVER.keys(): cfg.SOLVER.MAX_ITER = cfg.SOLVER.EPOCHS*cfg.EPOCH_SIZE\n",
    "    max_iter = cfg.SOLVER.MAX_ITER\n",
    "    start_iter = meta.get(\"iteration\",-1)+1 \n",
    "\n",
    "    if resume: \n",
    "        logger.info(f\"Resumed model: {meta.get('model_name','unknown')}\")\n",
    "        #override some configs from checkpoint in case you changed them\n",
    "        scheduler.milestones = cfg.SOLVER.STEPS\n",
    "        scheduler.gamma = cfg.SOLVER.GAMMA\n",
    "        scheduler.base_lrs = [cfg.SOLVER.BASE_LR for lr in scheduler.base_lrs]\n",
    "        scheduler.last_epoch = start_iter\n",
    "\n",
    "    writers = [\n",
    "        CommonMetricPrinter(max_iter),\n",
    "        JSONWriter(f\"{cfg.OUTPUT_DIR}/{model_context}-metrics.json\"),\n",
    "        TensorboardXWriter(cfg.OUTPUT_DIR),\n",
    "    ]\n",
    "\n",
    "    model.train() #set to training mode (PyTorch)\n",
    "    dataloader = build_detection_train_loader(cfg,mapper=DatasetPipeline(cfg,True))\n",
    "    logger.info(f\"Training: Start Iter {start_iter}, End Iter {max_iter}\")\n",
    "    with EventStorage(start_iter) as storage:\n",
    "        for iteration,data in zip(range(start_iter,max_iter),dataloader):\n",
    "            try:\n",
    "                iteration = iteration + 1\n",
    "                storage.step()\n",
    "                \n",
    "                loss_dict = model(data)\n",
    "                losses = sum(loss_dict.values())\n",
    "                assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "                loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
    "                losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "                if comm.is_main_process(): storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
    "                scheduler.step()\n",
    "\n",
    "                if iteration - start_iter > 5 and (iteration % cfg.PRINT_EVERY == 0 or iteration == max_iter):\n",
    "                    for writer in writers: writer.write()\n",
    "                checkpointer.step(iteration)\n",
    "                \n",
    "            except (Exception,KeyboardInterrupt) as e:\n",
    "                logger.info(\"ERROR! Dumping current model...\")\n",
    "                checkpointer.save(f\"{model_context}-{iteration}-interrupted\",iteration=iteration,model_name=model_context,min_val_loss=checkpointer.min_loss)\n",
    "                raise e\n",
    "    checkpointer.save(f\"{model_context}-{max_iter}-final\",iteration=max_iter,model_name=model_context,min_val_loss=checkpointer.min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.58it/s]\n\n\u001b[32m[06/16 12:00:12 detectron2]: \u001b[0mVal Loss: 0.381982684135437 @ Iteration 59999, Min Val Loss: 0.3896270990371704, saving model.\n\n\u001b[32m[06/16 12:00:13 d2.utils.events]: \u001b[0m eta: 8 days, 15:53:15  iter: 60000  total_loss: 0.302  loss_cls: 0.127  loss_box_reg: 0.168  loss_rpn_cls: 0.003  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:05:04 d2.utils.events]: \u001b[0m eta: 6 days, 7:46:33  iter: 60500  total_loss: 0.343  loss_cls: 0.130  loss_box_reg: 0.189  loss_rpn_cls: 0.003  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:09:53 d2.utils.events]: \u001b[0m eta: 6 days, 7:02:17  iter: 61000  total_loss: 0.368  loss_cls: 0.162  loss_box_reg: 0.184  loss_rpn_cls: 0.006  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:14:44 d2.utils.events]: \u001b[0m eta: 6 days, 7:34:07  iter: 61500  total_loss: 0.395  loss_cls: 0.161  loss_box_reg: 0.206  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:19:36 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 12:19:36 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 12:19:36 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 12:19:36 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 12:19:36 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.59it/s]\n\n\u001b[32m[06/16 12:21:25 detectron2]: \u001b[0mVal Loss: 0.39229947328567505 @ Iteration 61999, Min Val Loss: 0.381982684135437, did not save model.\n\n\u001b[32m[06/16 12:21:25 d2.utils.events]: \u001b[0m eta: 8 days, 17:05:04  iter: 62000  total_loss: 0.405  loss_cls: 0.165  loss_box_reg: 0.209  loss_rpn_cls: 0.005  loss_rpn_loc: 0.017  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:26:16 d2.utils.events]: \u001b[0m eta: 6 days, 7:15:16  iter: 62500  total_loss: 0.366  loss_cls: 0.146  loss_box_reg: 0.188  loss_rpn_cls: 0.004  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:31:06 d2.utils.events]: \u001b[0m eta: 6 days, 7:14:31  iter: 63000  total_loss: 0.362  loss_cls: 0.150  loss_box_reg: 0.204  loss_rpn_cls: 0.003  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:35:57 d2.utils.events]: \u001b[0m eta: 6 days, 7:23:30  iter: 63500  total_loss: 0.330  loss_cls: 0.134  loss_box_reg: 0.174  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:40:46 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 12:40:46 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 12:40:46 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 12:40:46 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 12:40:46 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:48<00:00,  9.60it/s]\n\n\u001b[32m[06/16 12:42:35 detectron2]: \u001b[0mVal Loss: 0.38889259099960327 @ Iteration 63999, Min Val Loss: 0.381982684135437, did not save model.\n\n\u001b[32m[06/16 12:42:36 d2.utils.events]: \u001b[0m eta: 8 days, 15:04:28  iter: 64000  total_loss: 0.343  loss_cls: 0.144  loss_box_reg: 0.185  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:47:26 d2.utils.events]: \u001b[0m eta: 6 days, 6:56:11  iter: 64500  total_loss: 0.406  loss_cls: 0.165  loss_box_reg: 0.210  loss_rpn_cls: 0.006  loss_rpn_loc: 0.018  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:52:15 d2.utils.events]: \u001b[0m eta: 6 days, 6:21:23  iter: 65000  total_loss: 0.382  loss_cls: 0.153  loss_box_reg: 0.201  loss_rpn_cls: 0.003  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 12:57:05 d2.utils.events]: \u001b[0m eta: 6 days, 6:15:57  iter: 65500  total_loss: 0.311  loss_cls: 0.137  loss_box_reg: 0.174  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:01:54 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 13:01:54 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 13:01:54 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 13:01:54 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 13:01:54 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.57it/s]\n\n\u001b[32m[06/16 13:03:43 detectron2]: \u001b[0mVal Loss: 0.37014180421829224 @ Iteration 65999, Min Val Loss: 0.381982684135437, saving model.\n\n\u001b[32m[06/16 13:03:44 d2.utils.events]: \u001b[0m eta: 8 days, 15:08:46  iter: 66000  total_loss: 0.358  loss_cls: 0.135  loss_box_reg: 0.193  loss_rpn_cls: 0.006  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:08:33 d2.utils.events]: \u001b[0m eta: 6 days, 5:59:50  iter: 66500  total_loss: 0.331  loss_cls: 0.123  loss_box_reg: 0.171  loss_rpn_cls: 0.006  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:13:24 d2.utils.events]: \u001b[0m eta: 6 days, 6:31:58  iter: 67000  total_loss: 0.373  loss_cls: 0.169  loss_box_reg: 0.174  loss_rpn_cls: 0.005  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:18:12 d2.utils.events]: \u001b[0m eta: 6 days, 5:28:49  iter: 67500  total_loss: 0.352  loss_cls: 0.141  loss_box_reg: 0.181  loss_rpn_cls: 0.004  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:23:04 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 13:23:04 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 13:23:04 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 13:23:04 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 13:23:04 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.59it/s]\n\n\u001b[32m[06/16 13:24:54 detectron2]: \u001b[0mVal Loss: 0.38023048639297485 @ Iteration 67999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 13:24:54 d2.utils.events]: \u001b[0m eta: 8 days, 15:58:02  iter: 68000  total_loss: 0.359  loss_cls: 0.152  loss_box_reg: 0.182  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:29:43 d2.utils.events]: \u001b[0m eta: 6 days, 5:28:59  iter: 68500  total_loss: 0.359  loss_cls: 0.148  loss_box_reg: 0.179  loss_rpn_cls: 0.004  loss_rpn_loc: 0.012  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:34:33 d2.utils.events]: \u001b[0m eta: 6 days, 6:03:43  iter: 69000  total_loss: 0.343  loss_cls: 0.140  loss_box_reg: 0.186  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:39:25 d2.utils.events]: \u001b[0m eta: 6 days, 6:50:21  iter: 69500  total_loss: 0.359  loss_cls: 0.146  loss_box_reg: 0.186  loss_rpn_cls: 0.003  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:44:12 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 13:44:12 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 13:44:12 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 13:44:12 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 13:44:12 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.59it/s]\n\n\u001b[32m[06/16 13:46:01 detectron2]: \u001b[0mVal Loss: 0.3897048532962799 @ Iteration 69999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 13:46:02 d2.utils.events]: \u001b[0m eta: 8 days, 13:08:13  iter: 70000  total_loss: 0.376  loss_cls: 0.156  loss_box_reg: 0.204  loss_rpn_cls: 0.004  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:50:53 d2.utils.events]: \u001b[0m eta: 6 days, 6:20:43  iter: 70500  total_loss: 0.386  loss_cls: 0.176  loss_box_reg: 0.196  loss_rpn_cls: 0.005  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 13:55:44 d2.utils.events]: \u001b[0m eta: 6 days, 6:29:54  iter: 71000  total_loss: 0.349  loss_cls: 0.139  loss_box_reg: 0.183  loss_rpn_cls: 0.003  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:00:35 d2.utils.events]: \u001b[0m eta: 6 days, 5:39:03  iter: 71500  total_loss: 0.368  loss_cls: 0.163  loss_box_reg: 0.184  loss_rpn_cls: 0.006  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:05:23 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 14:05:23 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 14:05:23 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 14:05:23 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 14:05:23 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.57it/s]\n\n\u001b[32m[06/16 14:07:13 detectron2]: \u001b[0mVal Loss: 0.39864709973335266 @ Iteration 71999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 14:07:13 d2.utils.events]: \u001b[0m eta: 8 days, 13:31:19  iter: 72000  total_loss: 0.336  loss_cls: 0.146  loss_box_reg: 0.176  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:12:05 d2.utils.events]: \u001b[0m eta: 6 days, 6:15:27  iter: 72500  total_loss: 0.359  loss_cls: 0.145  loss_box_reg: 0.194  loss_rpn_cls: 0.006  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:16:54 d2.utils.events]: \u001b[0m eta: 6 days, 5:08:47  iter: 73000  total_loss: 0.333  loss_cls: 0.142  loss_box_reg: 0.187  loss_rpn_cls: 0.006  loss_rpn_loc: 0.012  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:21:44 d2.utils.events]: \u001b[0m eta: 6 days, 5:18:10  iter: 73500  total_loss: 0.371  loss_cls: 0.135  loss_box_reg: 0.193  loss_rpn_cls: 0.003  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:26:34 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 14:26:34 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 14:26:34 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 14:26:34 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 14:26:34 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.58it/s]\n\n\u001b[32m[06/16 14:28:24 detectron2]: \u001b[0mVal Loss: 0.38332512974739075 @ Iteration 73999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 14:28:24 d2.utils.events]: \u001b[0m eta: 8 days, 13:33:36  iter: 74000  total_loss: 0.339  loss_cls: 0.129  loss_box_reg: 0.187  loss_rpn_cls: 0.004  loss_rpn_loc: 0.017  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:33:16 d2.utils.events]: \u001b[0m eta: 6 days, 5:55:10  iter: 74500  total_loss: 0.351  loss_cls: 0.142  loss_box_reg: 0.183  loss_rpn_cls: 0.005  loss_rpn_loc: 0.012  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:38:06 d2.utils.events]: \u001b[0m eta: 6 days, 5:08:45  iter: 75000  total_loss: 0.381  loss_cls: 0.163  loss_box_reg: 0.194  loss_rpn_cls: 0.004  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:42:55 d2.utils.events]: \u001b[0m eta: 6 days, 4:28:52  iter: 75500  total_loss: 0.320  loss_cls: 0.136  loss_box_reg: 0.168  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:47:45 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 14:47:45 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 14:47:45 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 14:47:45 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 14:47:45 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.56it/s]\n\n\u001b[32m[06/16 14:49:35 detectron2]: \u001b[0mVal Loss: 0.38701069355010986 @ Iteration 75999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 14:49:35 d2.utils.events]: \u001b[0m eta: 8 days, 13:19:39  iter: 76000  total_loss: 0.317  loss_cls: 0.123  loss_box_reg: 0.173  loss_rpn_cls: 0.002  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:54:25 d2.utils.events]: \u001b[0m eta: 6 days, 4:58:38  iter: 76500  total_loss: 0.342  loss_cls: 0.159  loss_box_reg: 0.174  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 14:59:17 d2.utils.events]: \u001b[0m eta: 6 days, 5:18:39  iter: 77000  total_loss: 0.328  loss_cls: 0.136  loss_box_reg: 0.184  loss_rpn_cls: 0.002  loss_rpn_loc: 0.011  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:04:08 d2.utils.events]: \u001b[0m eta: 6 days, 5:33:58  iter: 77500  total_loss: 0.418  loss_cls: 0.155  loss_box_reg: 0.216  loss_rpn_cls: 0.007  loss_rpn_loc: 0.017  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:08:58 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 15:08:58 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 15:08:58 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 15:08:58 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 15:08:58 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.57it/s]\n\n\u001b[32m[06/16 15:10:47 detectron2]: \u001b[0mVal Loss: 0.38333824276924133 @ Iteration 77999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 15:10:48 d2.utils.events]: \u001b[0m eta: 8 days, 12:35:43  iter: 78000  total_loss: 0.333  loss_cls: 0.143  loss_box_reg: 0.168  loss_rpn_cls: 0.003  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:15:38 d2.utils.events]: \u001b[0m eta: 6 days, 4:29:15  iter: 78500  total_loss: 0.327  loss_cls: 0.130  loss_box_reg: 0.174  loss_rpn_cls: 0.004  loss_rpn_loc: 0.012  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:20:29 d2.utils.events]: \u001b[0m eta: 6 days, 4:52:36  iter: 79000  total_loss: 0.400  loss_cls: 0.155  loss_box_reg: 0.188  loss_rpn_cls: 0.007  loss_rpn_loc: 0.017  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:25:19 d2.utils.events]: \u001b[0m eta: 6 days, 4:20:06  iter: 79500  total_loss: 0.368  loss_cls: 0.168  loss_box_reg: 0.176  loss_rpn_cls: 0.002  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:30:08 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 15:30:08 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 15:30:08 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 15:30:08 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 15:30:08 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.56it/s]\n\n\u001b[32m[06/16 15:31:57 detectron2]: \u001b[0mVal Loss: 0.38345855474472046 @ Iteration 79999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 15:31:58 d2.utils.events]: \u001b[0m eta: 8 days, 11:56:56  iter: 80000  total_loss: 0.319  loss_cls: 0.138  loss_box_reg: 0.162  loss_rpn_cls: 0.005  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:36:48 d2.utils.events]: \u001b[0m eta: 6 days, 3:59:11  iter: 80500  total_loss: 0.405  loss_cls: 0.177  loss_box_reg: 0.196  loss_rpn_cls: 0.007  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:41:40 d2.utils.events]: \u001b[0m eta: 6 days, 5:28:55  iter: 81000  total_loss: 0.359  loss_cls: 0.151  loss_box_reg: 0.177  loss_rpn_cls: 0.005  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:46:31 d2.utils.events]: \u001b[0m eta: 6 days, 4:33:45  iter: 81500  total_loss: 0.323  loss_cls: 0.135  loss_box_reg: 0.176  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:51:22 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 15:51:22 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 15:51:22 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 15:51:22 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 15:51:22 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.58it/s]\n\n\u001b[32m[06/16 15:53:11 detectron2]: \u001b[0mVal Loss: 0.37959423661231995 @ Iteration 81999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 15:53:11 d2.utils.events]: \u001b[0m eta: 8 days, 11:52:58  iter: 82000  total_loss: 0.285  loss_cls: 0.119  loss_box_reg: 0.166  loss_rpn_cls: 0.003  loss_rpn_loc: 0.015  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 15:58:01 d2.utils.events]: \u001b[0m eta: 6 days, 3:50:22  iter: 82500  total_loss: 0.362  loss_cls: 0.153  loss_box_reg: 0.198  loss_rpn_cls: 0.004  loss_rpn_loc: 0.012  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 16:02:50 d2.utils.events]: \u001b[0m eta: 6 days, 3:03:03  iter: 83000  total_loss: 0.363  loss_cls: 0.136  loss_box_reg: 0.201  loss_rpn_cls: 0.007  loss_rpn_loc: 0.016  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 16:07:41 d2.utils.events]: \u001b[0m eta: 6 days, 4:06:24  iter: 83500  total_loss: 0.380  loss_cls: 0.170  loss_box_reg: 0.193  loss_rpn_cls: 0.006  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 16:12:32 detectron2]: \u001b[0mPipeline: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[06/16 16:12:32 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 16:12:32 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 16:12:32 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 16:12:32 detectron2]: \u001b[0mCalculating Validation Loss...\n100%|██████████| 1046/1046 [01:49<00:00,  9.58it/s]\n\n\u001b[32m[06/16 16:14:21 detectron2]: \u001b[0mVal Loss: 0.38255733251571655 @ Iteration 83999, Min Val Loss: 0.37014180421829224, did not save model.\n\n\u001b[32m[06/16 16:14:21 d2.utils.events]: \u001b[0m eta: 8 days, 11:52:56  iter: 84000  total_loss: 0.359  loss_cls: 0.172  loss_box_reg: 0.189  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  lr: 0.000375  max_mem: 4884M\n\u001b[32m[06/16 16:18:36 detectron2]: \u001b[0mERROR! Dumping current model...\nProcess Process-4:\nProcess Process-9:\nProcess Process-5:\nProcess Process-2:\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f618d1adee0>\nTraceback (most recent call last):\n  File \"/home/interpause/miniconda3/envs/detectron/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/home/interpause/miniconda3/envs/detectron/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/home/interpause/miniconda3/envs/detectron/lib/python3.8/multiprocessing/process.py\", line 149, in join\n    res = self._popen.wait(timeout)\n  File \"/home/interpause/miniconda3/envs/detectron/lib/python3.8/multiprocessing/popen_fork.py\", line 47, in wait\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n  File \"/home/interpause/miniconda3/envs/detectron/lib/python3.8/multiprocessing/popen_fork.py\", line 27, in poll\n    pid, sts = os.waitpid(self.pid, flag)\nKeyboardInterrupt: \nProcess Process-8:\nProcess Process-6:\nProcess Process-7:\nProcess Process-1:\nProcess Process-3:\nTraceback (most recent call last):\nTraceback (most recent call last):\n"
    }
   ],
   "source": [
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "#cfg.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn-17999-best_val.pth\")\n",
    "do_train(cfg,model,\"ft-til_resnet101_rcnn_moda\",resume=False)\n",
    "#if this doesnt work correctly, check what model is set in the  file in /ckpts/last_checkpoint\n",
    "#I uploaded the model file to the google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32m[06/16 16:24:25 d2.data.datasets.coco]: \u001b[0mLoaded 1046 images in COCO format from modanet/nval.json\n\u001b[32m[06/16 16:24:25 d2.data.build]: \u001b[0mDistribution of instances among all 13 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    bag     | 432          |    belt    | 301          |   boots    | 239          |\n|  footwear  | 1612         |   outer    | 507          |   dress    | 293          |\n| sunglasses | 193          |   pants    | 450          |    top     | 729          |\n|   shorts   | 152          |   skirt    | 282          |  headwear  | 113          |\n| scarf/tie  | 107          |            |              |            |              |\n|   total    | 5410         |            |              |            |              |\u001b[0m\n\u001b[32m[06/16 16:24:25 d2.data.common]: \u001b[0mSerializing 1046 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/16 16:24:25 d2.data.common]: \u001b[0mSerialized dataset takes 1.25 MiB\n\u001b[32m[06/16 16:24:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 1046 images\n\u001b[32m[06/16 16:24:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/1046. 0.0984 s / img. ETA=0:01:42\n\u001b[32m[06/16 16:24:32 d2.evaluation.evaluator]: \u001b[0mInference done 63/1046. 0.0970 s / img. ETA=0:01:36\n\u001b[32m[06/16 16:24:37 d2.evaluation.evaluator]: \u001b[0mInference done 114/1046. 0.0976 s / img. ETA=0:01:31\n\u001b[32m[06/16 16:24:42 d2.evaluation.evaluator]: \u001b[0mInference done 167/1046. 0.0964 s / img. ETA=0:01:25\n\u001b[32m[06/16 16:24:47 d2.evaluation.evaluator]: \u001b[0mInference done 220/1046. 0.0960 s / img. ETA=0:01:19\n\u001b[32m[06/16 16:24:52 d2.evaluation.evaluator]: \u001b[0mInference done 272/1046. 0.0959 s / img. ETA=0:01:14\n\u001b[32m[06/16 16:24:57 d2.evaluation.evaluator]: \u001b[0mInference done 321/1046. 0.0968 s / img. ETA=0:01:10\n\u001b[32m[06/16 16:25:02 d2.evaluation.evaluator]: \u001b[0mInference done 373/1046. 0.0968 s / img. ETA=0:01:05\n\u001b[32m[06/16 16:25:07 d2.evaluation.evaluator]: \u001b[0mInference done 425/1046. 0.0968 s / img. ETA=0:01:00\n\u001b[32m[06/16 16:25:12 d2.evaluation.evaluator]: \u001b[0mInference done 477/1046. 0.0968 s / img. ETA=0:00:55\n\u001b[32m[06/16 16:25:17 d2.evaluation.evaluator]: \u001b[0mInference done 529/1046. 0.0968 s / img. ETA=0:00:50\n\u001b[32m[06/16 16:25:22 d2.evaluation.evaluator]: \u001b[0mInference done 579/1046. 0.0970 s / img. ETA=0:00:45\n\u001b[32m[06/16 16:25:27 d2.evaluation.evaluator]: \u001b[0mInference done 629/1046. 0.0973 s / img. ETA=0:00:40\n\u001b[32m[06/16 16:25:32 d2.evaluation.evaluator]: \u001b[0mInference done 681/1046. 0.0973 s / img. ETA=0:00:35\n\u001b[32m[06/16 16:25:37 d2.evaluation.evaluator]: \u001b[0mInference done 732/1046. 0.0974 s / img. ETA=0:00:30\n\u001b[32m[06/16 16:25:43 d2.evaluation.evaluator]: \u001b[0mInference done 779/1046. 0.0979 s / img. ETA=0:00:26\n\u001b[32m[06/16 16:25:48 d2.evaluation.evaluator]: \u001b[0mInference done 832/1046. 0.0977 s / img. ETA=0:00:21\n\u001b[32m[06/16 16:25:53 d2.evaluation.evaluator]: \u001b[0mInference done 885/1046. 0.0975 s / img. ETA=0:00:15\n\u001b[32m[06/16 16:25:58 d2.evaluation.evaluator]: \u001b[0mInference done 937/1046. 0.0974 s / img. ETA=0:00:10\n\u001b[32m[06/16 16:26:03 d2.evaluation.evaluator]: \u001b[0mInference done 990/1046. 0.0972 s / img. ETA=0:00:05\n\u001b[32m[06/16 16:26:08 d2.evaluation.evaluator]: \u001b[0mInference done 1042/1046. 0.0972 s / img. ETA=0:00:00\n\u001b[32m[06/16 16:26:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:42.090100 (0.098069 s / img per device, on 1 devices)\n\u001b[32m[06/16 16:26:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:41 (0.097145 s / img per device, on 1 devices)\n\u001b[32m[06/16 16:26:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n\u001b[32m[06/16 16:26:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions ...\nLoading and preparing results...\nDONE (t=0.01s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=1.77s).\nAccumulating evaluation results...\nDONE (t=0.39s).\n Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.790\n Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.816\n Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.801\n Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.784\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.757\n\u001b[32m[06/16 16:26:10 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n|:------:|:------:|:------:|:------:|:------:|:-----:|\n| 78.979 | 81.643 | 80.120 | 78.366 | 75.712 | 0.000 |\n\u001b[32m[06/16 16:26:10 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n| category   | AP     | category   | AP     | category   | AP     |\n|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n| bag        | 91.517 | belt       | 86.872 | boots      | 29.822 |\n| footwear   | 30.819 | outer      | 89.498 | dress      | 86.255 |\n| sunglasses | 95.231 | pants      | 95.938 | top        | 81.755 |\n| shorts     | 89.271 | skirt      | 85.822 | headwear   | 96.280 |\n| scarf/tie  | 67.648 |            |        |            |        |\n"
    }
   ],
   "source": [
    "#if do_train() hasnt been called (it loads the model) and you want to load the model\n",
    "if True:\n",
    "    from detectron2.checkpoint import DetectionCheckpointer\n",
    "    cfg.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn_moda-65999-best_val.pth\")\n",
    "    DetectionCheckpointer(model, cfg.OUTPUT_DIR).resume_or_load(cfg.MODEL.WEIGHTS, resume=False)\n",
    "\n",
    "do_test(cfg,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "cfg.MODEL.WEIGHTS = str(load_model_folder/\"ft-til_resnet101_rcnn-17999-best_val.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0 #max AP at cost of inference speed\n",
    "predictor = DefaultPredictor(cfg) #check what exactly default predictor does, and if it affects anything badly.\n",
    "#Are output boxes supposed to be rescaled? btw, the current pipeline resizes back to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes2xywh(boxes):\n",
    "    return [[x1,y1,x2-x1,y2-y1] for x1,y1,x2,y2 in boxes.tensor.tolist()]\n",
    "\n",
    "def convert_prediction(img_id,output):\n",
    "    outs = []\n",
    "    raw = output['instances']\n",
    "    boxes = boxes2xywh(raw.pred_boxes)\n",
    "    cats = raw.pred_classes.tolist()\n",
    "    scores = raw.scores.tolist()\n",
    "\n",
    "    for i in range(len(raw)):\n",
    "        outs.append({\n",
    "            'image_id':img_id,\n",
    "            'category_id':cats[i],\n",
    "            'bbox':boxes[i],\n",
    "            'score':scores[i]\n",
    "        })\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1000/1000 [02:44<00:00,  6.07it/s]\n"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "output = []\n",
    "for im_path in tqdm(list(test_imgs_folder.glob('*'))):\n",
    "    im = cv2.imread(im_path) #we are using cv2 here to be absolutely sure; this is what is in the documentation\n",
    "    output += convert_prediction(int(im_path.stem),predictor(im))\n",
    "\n",
    "with open('ans.json','w') as f:\n",
    "    json.dump(output,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "im = cv2.imread(train_imgs_folder/\"69.jpg\")\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "im_out = Image.fromarray(v.get_image()[:, :, ::-1]) #channels are reversed\n",
    "#display(im_out)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitdetectronconda1819ced9d1b04054b76de77970507a6d",
   "display_name": "Python 3.8.3 64-bit ('detectron': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}